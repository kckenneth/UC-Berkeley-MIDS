{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2 - Naive Bayes in Hadoop MR\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "In the live sessions for week 2 and week 3 you got some practice designing and debugging Hadoop Streaming jobs. In this homework we'll use Hadoop MapReduce to implement your first parallelized machine learning algorithm: Naive Bayes. As you develop your implementation you'll test it on a small dataset that matches the 'Chinese Example' in the _Manning, Raghavan and Shutze_ reading for Week 2. For the main task in this assignment you'll be working with a small subset of the Enron Spam/Ham Corpus. By the end of this assignment you should be able to:\n",
    "* __... describe__ the Naive Bayes algorithm including both training and inference.\n",
    "* __... perform__ EDA on a corpus using Hadoop MR.\n",
    "* __... implement__ parallelized Naive Bayes.\n",
    "* __... constrast__ partial, unordered and total order sort and their implementations in Hadoop Streaming.\n",
    "* __... explain__ how smoothing affects the bias and variance of a Multinomial Naive Bayes model.\n",
    "\n",
    "As always, your work will be graded both on the correctness of your output and on the clarity and design of your code. __Please refer to the `README` for homework submission instructions.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "Before starting, run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global vars (paths) - ADJUST AS NEEDED\n",
    "JAR_FILE = \"/usr/lib/hadoop-mapreduce/hadoop-streaming.jar\"\n",
    "HDFS_DIR = \"/user/root/HW2\"\n",
    "HOME_DIR = \"/media/notebooks/Assignments/HW2\" # FILL IN HERE eg. /media/notebooks/Assignments/HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save path for use in Hadoop jobs (-cmdenv PATH={PATH})\n",
    "from os import environ\n",
    "PATH  = environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "ENRON = HOME_DIR + \"/data/enronemail_1h.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Hadoop MapReduce Key Takeaways.  \n",
    "\n",
    "This assignment will be the only one in which you use Hadoop Streaming to implement a distributed algorithm. The key reason we continue to teach Hadoop streaming is because of the way it forces the programmer to think carefully about what is happening under the hood when you parallelize a calculation. This question will briefly highlight some of the most important concepts that you need to understand about Hadoop Streaming and MapReduce before we move on to Spark next week.   \n",
    "\n",
    "### Q1 Tasks:\n",
    "\n",
    "* __a) short response:__ What \"programming paradigm\" is Hadoop MapReduce based on? What are the main ideas of this programming paradigm and how does MapReduce exemplify these ideas?\n",
    "\n",
    "* __b) short response:__ What is the Hadoop Shuffle? When does it happen? Why is it potentially costly? Describe one specific thing we can we do to mitigate the cost associated with this stage of our Hadoop Streaming jobs.\n",
    "\n",
    "* __c) short response:__ In Hadoop Streaming why do the input and output record format of a combiner script have to be the same? [__`HINT`__ _what level of combining does the framework guarantee? what is the relationship between the record format your mapper emits and the format your reducer expects to receive?_]\n",
    "\n",
    "* __d) short response:__ To what extent can you control the level of parallelization of your Hadoop Streaming jobs? Please be specific.\n",
    "\n",
    "* __e) short response:__ What change in the kind of computing resources available prompted the creation of parallel computation frameworks like Hadoop? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Student Answers:\n",
    "\n",
    "> __a)__ Hadoop MapReduce is based on **functional programming paradigm** in which mathematical evaluation is paramount without changing the state or mutable data. In contrast of *declarative programming* in which variables are declared and can be changed either locally or globally, functional programming evaluates a given argument mathematically and generates the output which does not change upon second time execution. MapReduce achieves this paradigm by divide and conquer method, which divides the data into many manageable partitions and executes mathematical operations in parallel. A key feature is the higher-order function that can accept the function as an argument and execute accordingly. Therefore, **Map** and **Reduce** can be attributed as higher-order functions that can accept other functions as arguments. Hadoop is a unique implementation of MapReduce with additional features such as distributed file system to hold a copious amount of data and resist data degeneration by duplicating in several nodes (HDFS). \n",
    "\n",
    "> __b)__ Hadoop shuffling is the transfer of output data from the mapper to the reducer. Because of the multiple outputs from multiple mappers, it's potentially costly when the reducer merges all partitioned data and combined the output. Reducer phase looks like a bottle neck where all processed data are coming into. The incoming data for mappers are future keys (eg, \"birds\", \"fly\"), but the output from mappers are {\"birds\": 20, \"fly\": 10} with respective values. It's costly especially when the data is large and all data on the fly are being stored and processed in memory and might run out of memory. In order to mitigate the cost, we can introduce in-mapper sorting function (combiner) that would sort each mapper output locally before relaying to the reducer, eg., aggregating the values of the same key in in-mapper phase, thereby reducing the network traffic between nodes. Therefore combiner phase can be thought of as \"mini-reducer\" phase. When combining or reducing at the reducer phase, we can also implement *merge sort* approach which will greatly reduce the cost and synchronize the communication during the shuffling phase. \n",
    "\n",
    "> __c)__ Since the combiners serve as local aggregators in order to alleviate the burden on reducers, combiners can be considered as \"mini-reducers\". However combiners are not compulsory steps in order to process the data at the reducers phase. Combiners are optmization steps. Rightly so, combiners might not receive all the keys-values pairs locally. Therefore, it's essential to have the input and ouput format of the combiners to be the same so that reducers can recognize the input format even if some data are not processed by combiners and coming directly from the mappers. MapReduce framework guarantee at least in-mapper combining level. The relationship between the mappers and the reducers is consistent in terms of key-value pairs. The reducer receives the intermediate data, sorted by the key. However there's no guarantee that ordering relationship for keys across different reducers. \n",
    "\n",
    "> __d)__ We can control the level of parallelization by adjusting the number of mappers and reducers. Since the parallelization comes with communication costs, the higher the level of parallelization (number of tasks), the higher the cost, and ultimately results in diminishing return. \n",
    "\n",
    "> __e)__ The transition from single core CPU to multi-core CPUs prompted the creation of parallel computation. Hadoop is a combination of parallel computation with additional features such as distributed file system (HDFS), thereby achieving the scalability and data integrity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: MapReduce Design Patterns.  \n",
    "\n",
    "In the last two live sessions and in your readings from Lin & Dyer you encountered a number of techniques for manipulating the logistics of a MapReduce implementation to ensure that the right information is available at the right time and location. In this question we'll review a few of the key techniques you learned.   \n",
    "\n",
    "### Q2 Tasks:\n",
    "\n",
    "* __a) short response:__ What are counters (in the context of Hadoop Streaming)? How are they useful? What kinds of counters does Hadoop provide for you? How do you create your own custom counter?\n",
    "\n",
    "* __b) short response:__ What are composite keys? How are they useful? How are they related to the idea of custom partitioning?\n",
    "\n",
    "* __c) short response:__ What is the order inversion pattern? What problem does it help solve? How do we implement it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Student Answers:\n",
    "\n",
    "> __a)__ **Counters** are as the name suggests counting events during MapReduce execution, eg, number of successful tasks, number of corrupt records, number of times a certain conditions is met. Hadoop provides a lightweight API counters such as Job Tracking counters, file system counters, Job counter and so on. We can also create our own custom counters by printing incremental outputs under certain conditions in the mapper, combiner, or reducer scripts eg, `if (counter % 10000 == 0): print(\"{} record mapped\".format(counter))`\n",
    "\n",
    "> __b)__ A composite key is when the value-to-key conversion design pattern is employed. Eg, when the part of the value from the mapper function is moved to the existing key, creating an intermediate key, eg. {sensorID(key): timestamp, reading(value)} --> {sensorID, timestamp (key): reading(value)}. In this example, sensorID and timestamp become **a composite key**. Composite keys are useful because they provide an intermediate key sort and any other value emitted by the mapper function, which can now convert to the key. Secondary sorting can then be performed to support custom partitioning.\n",
    "\n",
    "> __c)__ The **order inversion** pattern takes advantage of the sorting phase of the mapper and executes the aggregate statistics which will make up the special key-value pair. For example, if we want to calculate the relative frequency of certain word pairs on either of the base words, we need the total number of the base word in the first place. Since the reducer is executing aggregate statistics, in order to calculate the relative frequency, we need to emit the total number of the base word to the reducer first. Therefore, as the procedure is implied, this pattern is called **order inversion** pattern.  \n",
    ">  \n",
    "> To implement as an example, we can imagine that we want to calculate the relative frequency of word pairs `(\"birds\", \"fly\")` among all the `(\"birds\")` word in the document. First we will create a special key-value pair, e.g, `(\"birds\", \"*\")` which is a total aggregate value of all the `(\"birds\")` words. The special key-value pair is sent to the reducer ahead of the time before the normal key-value pairs are sent, eg, `(\"birds\", \"fly\")`, `(\"birds\", \"perch\")`, `(\"birds\", \"dive\")`. Therefore, order inversion helps solve the problem of identifying the relative frequency, which requires the total number of words to use as a denominator. For example, the special key-value pair is sent to the reducer by hash function to make sure that the same first words are emitted to the same reducer, here `(\"birds\")`. The intermediate key `birds(key)` is shuffled to the same reducer by hashcode of the key modulo the number of reducers. When the special key-value `(\"birds\", \"*\")` pairs arrives at the reducer, which will check against the 2nd word `\"*\"`. If the current word in the reducer is not equal to the special character `\"*\"`, it will reset its counter value and sum up all the values to obtain the total number of the normal key-value pairs. By doing so, the reducer can calculate the relative frequency of the word pairs, e.g., `(\"birds\", \"fly\")` word pair is commonly found and makes up 50% of the entire `(\"birds\")` word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Understanding Total Order Sort\n",
    "\n",
    "The key challenge in distributed computing is to break a problem into a set of sub-problems that can be performed without communicating with each other. Ideally, we should be able to define an arbirtary number of splits and still get the right result, but that is not always possible. Parallelization becomes particularly challenging when we need to make comparisons between records, for example when sorting. Total Order Sort allows us to order large datasets in a way that enables efficient retrieval of results. Before beginning this assignment, make sure you have read and understand the [Total Order Sort Notebook](https://github.com/UCB-w261/main/tree/master/HelpfulResources/TotalSortGuide/_total-sort-guide-spark2.01-JAN27-2017.ipynb). You can skip the first two MRJob sections, but the rest of section III and all of section IV are **very** important (and apply to Hadoop Streaming) so make sure to read them closely. Feel free to read the Spark sections as well but you won't be responsible for that material until later in the course. To verify your understanding, answer the following questions.\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ What is the difference between a partial sort, an unordered total sort, and a total order sort? From the programmer's perspective, what does total order sort allow us to do that we can't with unordered total? Why is this important with large datasets?\n",
    "\n",
    "* __b) short response:__ Which phase of a MapReduce job is leveraged to implement Total Order Sort? Which default behaviors must be changed. Why must they be changed?\n",
    "\n",
    "* __c) short response:__ Describe in words how to configure a Hadoop Streaming job for the custom sorting and partitioning that is required for Total Order Sort.  \n",
    "\n",
    "* __d) short response:__ Explain why we need to use an inverse hash code function.\n",
    "\n",
    "* __e) short response:__ Where does this function need to be located so that a Total Order Sort can be performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "\n",
    "> __a)__ **A partial sort** is a key-sorted mapper output in each partition, but not across all partitions. eg  \n",
    "> `mapperA = (A, B, H)`,  \n",
    "> `mapperB = (C, D, I)`,  \n",
    "> `mapperC = (E, F, G)`  \n",
    "> We can see that in each partition, keys are sorted. However, when the mapperA and mapperB are compared, they need to be sorted again. Therefore it is a partial sort in the mapper function.  \n",
    ">  \n",
    "> **An unordered total sort** is a sorted mapper output in each and across all partitions. eg.,  \n",
    "> `mapperA = (G, H, I)`,  \n",
    "> `mapperB = (A, B, C)`,  \n",
    "> `mapperC = (D, E, F)`  \n",
    "> In unordered total sort, keys are sorted across all mappers. However when we want to compare mapperA and mapperB, we would have expected that mapperB should be the first to be **in order**. Therefore this approach is called **unordered total sort**.  \n",
    ">  \n",
    "> **A total order sort** is a completely sorted order not only in keys within each partition and across all partitions, but all partitions are also sorted.  \n",
    "> `mapperA = (A, B, C)`,  \n",
    "> `mapperB = (D, E, F)`,  \n",
    "> `mapperC = (G, H, I)`  \n",
    "> In total sort order, all keys are sorted within each partition and across all partitions. In addition, all partitions are sorted as well.  \n",
    ">  \n",
    "> From the programmer's perspective, total sort order is very crucial with respect to the memory and space constraint. By having a total sort order, we can easily retrieve the top partition and analyze the data in order. Or we can also retrieve the last partition to identify the lowest denominator in our data analysis. This is especially important in large dataset because we don't know which partition has the data we want to analyze and it can cause more overheads for reading and writing a large datafile in and out of memory. \n",
    "\n",
    "> __b)__ Shuffle phase is used to implement total order sort. Hadoop's default behavior is only a partial sort which is keys are sorted within each partitions. However partitions are not sorted. They must be changed so that a file can be easily retrieved and analyzed, based on the total sort order, especially a large data file. The other behavior in reducer also needs to change, i.e., the keys used to partition needs to be dropped. Reducers that we used so far, directly execute the record format from the mappers. However due to the partition information coming with key-value pairs, reducers need to drop the partition key (eg, hash function from the character of the word), and resume its execution with an original key-value pairs.  \n",
    "\n",
    "> __c)__ Since Hadoop only provides a partial sort, we need to create a custom sorting and partitioning.  \n",
    "> 1. Create in-mapper function that will custom-sort and partition based on the mapper key-value output. eg., mapper key can be transformed into hashcode by modulo the number of reducers or partitions we're going to create. For sorting a dictionary, we would create 26 partitions (reducers) to send all the same alphabets to the same reducer.  \n",
    "> 2. Remove hashcode in reducers internally and retrieve original key-value pairs.  \n",
    "> 3. Post-processing step to sort all the partitions. We have so far created an unordered total sort. But the sorted words that begin with a character, **`a`**, might be in partition 4 (e.g., part-00004). So we'd do the partition sort as post-process, which requires one record from each partition output and constructs an ordering function among all partitions. \n",
    "\n",
    "> __d)__ Usually, when we hash the partition key, the purpose is to generate the partition index for the key and the record will be emitted to the partition with respective to the index. However sometimes we might want to keep the partition keys in order and want to partition based on a desired index. **Inverse hashcode function** allows us to take the desired partition index and total number of partitions as inputs. It then computes a partition key based on our desired index. \n",
    "\n",
    "> __e)__ This inverse hashcode function needs to be implemented in Partitioner to create a total order sort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Data\n",
    "For the main task in this portion of the homework you will train a classifier to determine whether an email represents spam or not. You will train your Naive Bayes model on a 100 record subset of the Enron Spam/Ham corpus available in the HW2 data directory (__`HW2/data/enronemail_1h.txt`__).\n",
    "\n",
    "__Source:__   \n",
    "The original data included about 93,000 emails which were made public after the company's collapse. There have been a number raw and preprocessed versions of this corpus (including those available [here](http://www.aueb.gr/users/ion/data/enron-spam/index.html) and [here](http://www.aueb.gr/users/ion/publications.html)). The subset we will use is limited to emails from 6 Enron employees and a number of spam sources. It is part of [this data set](http://www.aueb.gr/users/ion/data/enron-spam/) which was created by researchers working on personlized Bayesian spam filters. Their original publication is [available here](http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf). __`IMPORTANT!`__ _For this homework please limit your analysis to the 100 email subset which we provide. No need to download or run your analysis on any of the original datasets, those links are merely provided as context._\n",
    "\n",
    "__Preprocessing:__  \n",
    "For their work, Metsis et al. (the authors) appeared to have pre-processed the data, not only collapsing all text to lower-case, but additionally separating \"words\" by spaces, where \"words\" unfortunately include punctuation. As a concrete example, the sentence:  \n",
    ">  `Hey Jon, I hope you don't get lost out there this weekend!`  \n",
    "\n",
    "... would have been reduced by Metsis et al. to the form:  \n",
    "> `hey jon , i hope you don ' t get lost out there this weekend !` \n",
    "\n",
    "... so we have reverted the data back toward its original state, removing spaces so that our sample sentence would now look like:\n",
    "> `hey jon, i hope you don't get lost out there this weekend!`  \n",
    "\n",
    "Thus we have at least preserved contractions and other higher-order lexical forms. However, one must be aware that this reversion is not complete, and that some object (specifically web sites) will be ill-formatted, and that all text is still lower-cased.\n",
    "\n",
    "\n",
    "__Format:__   \n",
    "All messages are collated to a tab-delimited format:  \n",
    "\n",
    ">    `ID \\t SPAM \\t SUBJECT \\t CONTENT \\n`  \n",
    "\n",
    "where:  \n",
    ">    `ID = string; unique message identifier`  \n",
    "    `SPAM = binary; with 1 indicating a spam message`  \n",
    "    `SUBJECT = string; title of the message`  \n",
    "    `CONTENT = string; content of the message`   \n",
    "    \n",
    "Note that either of `SUBJECT` or `CONTENT` may be \"NA\", and that all tab (\\t) and newline (\\n) characters have been removed from both of the `SUBJECT` and `CONTENT` columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t christmas tree farm pictures\tNA\n",
      "0001.1999-12-10.kaminski\t0\t re: rankings\t thank you.\n",
      "0001.2000-01-17.beck\t0\t leadership development pilot\t\" sally:  what timing, ask and you shall receiv\n",
      "0001.2000-06-06.lokay\t0\t\" key dates and impact of upcoming sap implementation over the next few week\n",
      "0001.2001-02-07.kitchen\t0\t key hr issues going forward\t a) year end reviews-report needs generating \n"
     ]
    }
   ],
   "source": [
    "# take a look at the first 100 characters of the first 5 records (RUN THIS CELL AS IS)\n",
    "!head -n 5 {ENRON} | cut -c-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 /media/notebooks/Assignments/HW2/data/enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# see how many messages/lines are in the file \n",
    "#(this number may be off by 1 if the last line doesn't end with a newline)\n",
    "!wc -l {ENRON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/root/HW2': File exists\n"
     ]
    }
   ],
   "source": [
    "# make the HDFS directory if it doesn't already exist\n",
    "!hdfs dfs -mkdir {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the data into HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -copyFromLocal {ENRON} {HDFS_DIR}/enron.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:  Enron Ham/Spam EDA.\n",
    "Before building our classifier, lets get aquainted with our data. In particular, we're interested in which words occur more in spam emails than in real emails. In this question you'll implement two Hadoop MapReduce jobs to count and sort word occurrences by document class. You'll also learn about two new Hadoop streaming parameters that will allow you to control how the records output of your mappers are partitioned for reducing on separate nodes. \n",
    "\n",
    "__`IMPORTANT NOTE:`__ For this question and all subsequent items, you should include both the subject and the body of the email in your analysis (i.e. concatetate them to get the 'text' of the document).\n",
    "\n",
    "### Q4 Tasks:\n",
    "* __a) code:__ Complete the missing components of the code in __`EnronEDA/mapper.py`__ and __`EnronEDA/reducer.py`__ to create a Hadoop MapReduce job that counts how many times each word in the corpus occurs in an email for each class. Pay close attention to the data format specified in the docstrings of these scripts _-- there are a number of ways to accomplish this task, we've chosen this format to help illustrate a technique in `part e`_. Run the provided unit tests to confirm that your code works as expected then run the provided Hadoop streaming command to apply your analysis to the Enron data.\n",
    "\n",
    "\n",
    "* __b) code + short response:__ How many times does the word \"__assistance__\" occur in each class? (`HINT:` Use a `grep` command to read from the results file you generated in '`a`' and then report the answer in the space provided.)\n",
    "\n",
    "\n",
    "* __c) short response:__ Would it have been possible to add some sorting parameters to the Hadoop streaming command that would cause our `part a` results to be sorted by count? Briefly explain why or why not.\n",
    "\n",
    "\n",
    "* __d) code + short response:__ Write a second Hadoop MapReduce job to sort the output of `part a` first by class and then by count. Run your job and save the results to a local file using the provide code. Then describe in words how you would go about printing the top 10 words in each class given this sorted output. (`HINT 1:` _remember that you can simply pass the `part a` output directory to the input field of this job; `HINT 2:` since this task is just reodering the records from `part a` we don't need to write a mapper or reducer, just use `/bin/cat` for both_)\n",
    "\n",
    "\n",
    "* __ e) code:__ A more efficient alternative to '`grep`-ing' for the top 10 words in each class would be to use the Hadoop framework to separate records from each class into its own partition so that we can just read the top lines in each. Edit your job from ` part d` to specify 2 reduce tasks and to tell Hadoop to partition based on the second field (which indicates spam/ham in our data). Your code should maintain the secondary sort -- that is each partition should list words from most to least frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __b)__ The word \"assistance\" was found __8__ times in spam emails and **2** times in regular emails. \n",
    "\n",
    "> __c)__ No. Since the mapper and the reducer we implemented has a record format similar to a tuple (word, class, 1) and the count hasn't been combined. Therefore we won't be able to sort the words by aggregate counts yet. \n",
    "\n",
    "> __d)__ Since we partitioned into two, spam and ham emails and combined them in order when we saved (eg, > EnronEDA/sorted_results.txt), we can call the top word by  \n",
    "> `!awk '$2==0' EnronEDA/sorted_results.txt | head -n 10` for regular emails  \n",
    "> `!awk '$2==1' EnronEDA/sorted_results.txt | head -n 10` for spam emails  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - do your work in the provided scripts then RUN THIS CELL AS IS\n",
    "!chmod a+x EnronEDA/mapper.py\n",
    "!chmod a+x EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\t1\t1\n",
      "body\t1\t1\n",
      "title\t0\t1\n",
      "body\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/mapper.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"d1\t1\ttitle\tbody\\nd2\t0\ttitle\tbody\" | EnronEDA/mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\t1\t1\n",
      "one\t0\t2\n",
      "two\t0\t1\n"
     ]
    }
   ],
   "source": [
    "# part a - unit test EnronEDA/reducer.py (RUN THIS CELL AS IS)\n",
    "!echo -e \"one\t1\t1\\none\t0\t1\\none\t0\t1\\ntwo\t0\t1\" | EnronEDA/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6446478263860175404.jar tmpDir=null\n",
      "19/01/16 23:38:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:38:03 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:38:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:38:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:38:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0033\n",
      "19/01/16 23:38:05 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0033\n",
      "19/01/16 23:38:05 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0033/\n",
      "19/01/16 23:38:05 INFO mapreduce.Job: Running job: job_1547262255608_0033\n",
      "19/01/16 23:38:13 INFO mapreduce.Job: Job job_1547262255608_0033 running in uber mode : false\n",
      "19/01/16 23:38:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:38:22 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/16 23:38:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:38:31 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/01/16 23:38:33 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:38:33 INFO mapreduce.Job: Job job_1547262255608_0033 completed successfully\n",
      "19/01/16 23:38:33 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=369010\n",
      "\t\tFILE: Number of bytes written=1335698\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217061\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12805\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=14535\n",
      "\t\tTotal time spent by all map tasks (ms)=12805\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14535\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12805\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14535\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13112320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=14883840\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=369022\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=369022\n",
      "\t\tReduce input records=31490\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=62980\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=171\n",
      "\t\tCPU time spent (ms)=5420\n",
      "\t\tPhysical memory (bytes) snapshot=982560768\n",
      "\t\tVirtual memory (bytes) snapshot=5485821952\n",
      "\t\tTotal committed heap usage (bytes)=742391808\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216847\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "19/01/16 23:38:33 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part a - Hadoop streaming job (RUN THIS CELL AS IS)\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part a - retrieve results from HDFS & copy them into a local file (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/eda-output/part-0000* > EnronEDA/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t1\t8\n",
      "assistance\t0\t2\n"
     ]
    }
   ],
   "source": [
    "# part b - write your grep command here\n",
    "!grep 'assistance' EnronEDA/results.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d/e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-sort-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7360827518110123786.jar tmpDir=null\n",
      "19/01/16 23:39:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:39:01 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:39:02 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "19/01/16 23:39:02 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:39:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0034\n",
      "19/01/16 23:39:03 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0034\n",
      "19/01/16 23:39:03 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0034/\n",
      "19/01/16 23:39:03 INFO mapreduce.Job: Running job: job_1547262255608_0034\n",
      "19/01/16 23:39:11 INFO mapreduce.Job: Job job_1547262255608_0034 running in uber mode : false\n",
      "19/01/16 23:39:11 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:39:19 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/16 23:39:20 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:39:28 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "19/01/16 23:39:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:39:30 INFO mapreduce.Job: Job job_1547262255608_0034 completed successfully\n",
      "19/01/16 23:39:31 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=88743\n",
      "\t\tFILE: Number of bytes written=772020\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=70789\n",
      "\t\tHDFS: Number of bytes written=76611\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12963\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13310\n",
      "\t\tTotal time spent by all map tasks (ms)=12963\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13310\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12963\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13310\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13274112\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13629440\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6060\n",
      "\t\tMap output records=6060\n",
      "\t\tMap output bytes=76611\n",
      "\t\tMap output materialized bytes=88755\n",
      "\t\tInput split bytes=238\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6060\n",
      "\t\tReduce shuffle bytes=88755\n",
      "\t\tReduce input records=6060\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=12120\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=148\n",
      "\t\tCPU time spent (ms)=4740\n",
      "\t\tPhysical memory (bytes) snapshot=957214720\n",
      "\t\tVirtual memory (bytes) snapshot=5495443456\n",
      "\t\tTotal committed heap usage (bytes)=670040064\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=70551\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=76611\n",
      "19/01/16 23:39:31 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part d/e - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=3 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k3,3nr\" \\\n",
    "  -D mapreduce.partition.keypartitioner.options=\"-k2,2\" \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/eda-output \\\n",
    "  -output {HDFS_DIR}/eda-sort-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== part-00000=====\n",
      "\n",
      "the\t0\t549\t\n",
      "to\t0\t398\t\n",
      "ect\t0\t382\t\n",
      "and\t0\t278\t\n",
      "of\t0\t230\t\n",
      "hou\t0\t206\t\n",
      "a\t0\t196\t\n",
      "in\t0\t182\t\n",
      "for\t0\t170\t\n",
      "on\t0\t135\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "===== part-00001=====\n",
      "\n",
      "the\t1\t698\t\n",
      "to\t1\t566\t\n",
      "and\t1\t392\t\n",
      "your\t1\t357\t\n",
      "a\t1\t347\t\n",
      "you\t1\t345\t\n",
      "of\t1\t336\t\n",
      "in\t1\t236\t\n",
      "for\t1\t204\t\n",
      "com\t1\t153\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# part e - view the top 10 records from each partition (RUN THIS CELL AS IS)\n",
    "for idx in range(2):\n",
    "    print(f\"\\n===== part-0000{idx}=====\\n\")\n",
    "    !hdfs dfs -cat {HDFS_DIR}/eda-sort-output/part-0000{idx} | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Expected output:__\n",
    "<table>\n",
    "<th>part-00000:</th>\n",
    "<th>part-00001:</th>\n",
    "<tr><td><pre>\n",
    "the\t0\t549\t\n",
    "to\t0\t398\t\n",
    "ect\t0\t382\t\n",
    "and\t0\t278\t\n",
    "of\t0\t230\t\n",
    "hou\t0\t206\t\n",
    "a\t0\t196\t\n",
    "in\t0\t182\t\n",
    "for\t0\t170\t\n",
    "on\t0\t135\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t1\t698\t\n",
    "to\t1\t566\t\n",
    "and\t1\t392\t\n",
    "your\t1\t357\t\n",
    "a\t1\t347\t\n",
    "you\t1\t345\t\n",
    "of\t1\t336\t\n",
    "in\t1\t236\t\n",
    "for\t1\t204\t\n",
    "com\t1\t153\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Counters and Combiners.\n",
    "Tuning the number of mappers & reducers is helpful to optimize very large distributed computations. Doing so successfully requires a thorough understanding of the data size at each stage of the job. As you learned in the week3 live session, counters are an invaluable resource for understanding this kind of detail. In this question, we will take the EDA performed in Question 4 as an opportunity to illustrate some related concepts.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response:__ Read the Hadoop output from your job in Question 4a to report how many records are emitted by the mappers and how many records are received be the reducers. In the context of word counting what does this number represent practically?\n",
    "\n",
    "* __b) code:__ Note that we wrote the reducer in question 4a such that the input and output record format is identical. This makes it easy to use the same reducer script as a combiner. In the space provided below, write the Hadoop Streaming command to re-run your job from question 4a with this combining added.\n",
    "\n",
    "* __c) short response__: Report the number of records emitted by your mappers in part b and the number of records received by your reducers. Compare your results here to what you saw in part a. Explain.\n",
    "\n",
    "* __d) short response__: Describe a scenario where using a combiner would _NOT_ improve the efficiency of the shuffle stage. Explain. [__`BONUS:`__ how does increasing the number of mappers affect the usefulness of a combiner?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ There are **31490** records emitted by the mappers and the same amount of records are received by the reducers. The 31490 records represent the total number of word occurrences in all 100 emails. \n",
    "\n",
    "> __c)__ With using reducer.py as *combiner* one step above, the number of records emitted by the mappers is still __31490__ but the number of records received by the reducers decreased to **7648**. It is because the combiner serves as a _**mini-reducer**_. We can also see that the output emitted by the combiner and the records received by the reducers is the same 7648. \n",
    "\n",
    "> __d)__ When the entire corpus of the documents, texts, emails, etc is very common with unique words, i.e., occurence of each word = 1, the presence of combiner (in-mapper function) would not improve the efficiency of the shuffle stage. The difference between the records emitted by the mapper and that of the combiner will be relatively small.  \n",
    "> In addition, in any condition, increasing the number of mappers inversely affects the usefulness of a combiner because the combiner main purpose is to aggregate the same key one step ahead of the reducers. Therefore, by having more mappers it would defeat the purpose of combiners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/eda-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob786558373979355867.jar tmpDir=null\n",
      "19/01/16 23:40:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:40:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:40:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:40:20 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:40:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0035\n",
      "19/01/16 23:40:21 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0035\n",
      "19/01/16 23:40:21 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0035/\n",
      "19/01/16 23:40:21 INFO mapreduce.Job: Running job: job_1547262255608_0035\n",
      "19/01/16 23:40:29 INFO mapreduce.Job: Job job_1547262255608_0035 running in uber mode : false\n",
      "19/01/16 23:40:29 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:40:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:40:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:40:47 INFO mapreduce.Job: Job job_1547262255608_0035 completed successfully\n",
      "19/01/16 23:40:48 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=102833\n",
      "\t\tFILE: Number of bytes written=804768\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217061\n",
      "\t\tHDFS: Number of bytes written=70551\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13482\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=13389\n",
      "\t\tTotal time spent by all map tasks (ms)=13482\n",
      "\t\tTotal time spent by all reduce tasks (ms)=13389\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13482\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=13389\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13805568\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=13710336\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=31490\n",
      "\t\tMap output bytes=306018\n",
      "\t\tMap output materialized bytes=102845\n",
      "\t\tInput split bytes=214\n",
      "\t\tCombine input records=31490\n",
      "\t\tCombine output records=7648\n",
      "\t\tReduce input groups=5065\n",
      "\t\tReduce shuffle bytes=102845\n",
      "\t\tReduce input records=7648\n",
      "\t\tReduce output records=6060\n",
      "\t\tSpilled Records=15296\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=125\n",
      "\t\tCPU time spent (ms)=5280\n",
      "\t\tPhysical memory (bytes) snapshot=886050816\n",
      "\t\tVirtual memory (bytes) snapshot=5486940160\n",
      "\t\tTotal committed heap usage (bytes)=738197504\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=216847\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=70551\n",
      "19/01/16 23:40:48 INFO streaming.StreamJob: Output directory: /user/root/HW2/eda-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your Hadoop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files EnronEDA/reducer.py,EnronEDA/mapper.py \\\n",
    "  -mapper mapper.py \\\n",
    "  -combiner reducer.py \\\n",
    "  -reducer reducer.py \\\n",
    "  -input {HDFS_DIR}/enron.txt \\\n",
    "  -output {HDFS_DIR}/eda-output \\\n",
    "  -numReduceTasks 2 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Document Classification Task Overview.\n",
    "The week 2 assigned reading from Chapter 13 of _Introduction to Information Retrieval_ by Manning, Raghavan and Schutze provides a thorough introduction to the document classification task and the math behind Naive Bayes. In this question we'll use the example from Table 13.1 (reproduced below) to 'train' an unsmoothed Multinomial Naive Bayes model and classify a test document by hand.\n",
    "\n",
    "<table>\n",
    "<th>DocID</th>\n",
    "<th>Class</th>\n",
    "<th>Subject</th>\n",
    "<th>Body</th>\n",
    "<tr><td>Doc1</td><td>1</td><td></td><td>Chinese Beijing Chinese</td></tr>\n",
    "<tr><td>Doc2</td><td>1</td><td></td><td>Chinese Chinese Shanghai</td></tr>\n",
    "<tr><td>Doc3</td><td>1</td><td></td><td>Chinese Macao</td></tr>\n",
    "<tr><td>Doc4</td><td>0</td><td></td><td>Tokyo Japan Chinese</td></tr>\n",
    "</table>\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) short response:__ Equation 13.3 in Manning, Raghavan and Shutze shows how a Multinomial Naive Bayes model classifies a document. It predicts the class, $c$, for which the estimated conditional probability of the class given the document's contents,  $\\hat{P}(c|d)$, is greatest. In this equation what two pieces of information are required to calculate  $\\hat{P}(c|d)$? Your answer should include both mathematical notatation and verbal explanation.\n",
    "\n",
    "\n",
    "* __b) short response:__ The Enron data includes two classes of documents: `spam` and `ham` (they're actually labeled `1` and `0`). In plain English, explain what  $\\hat{P}(c)$ and   $\\hat{P}(t_{k} | c)$ mean in the context of this data. How will we would estimate these values from a training corpus? How many passes over the data would we need to make to retrieve this information for all classes and all words?\n",
    "\n",
    "\n",
    "* __c) hand calculations:__ Above we've reproduced the document classification example from the textbook (we added an empty subject field to mimic the Enron data format). Remember that the classes in this \"Chinese Example\" are `1` (about China) and `0` (not about China). Calculate the class priors and the conditional probabilities for an __unsmoothed__ Multinomial Naive Bayes model trained on this data. Show the calculations that lead to your result using markdown and $\\LaTeX$ in the space provided or by embedding an image of your hand written work. [`NOTE:` _Your results should NOT match those in the text -- they are training a model with +1 smoothing you are training a model without smoothing_]\n",
    "\n",
    "\n",
    "* __d) hand calculations:__ Use the model you trained to classify the following test document: `Chinese Chinese Chinese Tokyo Japan`. Show the calculations that lead to your result using markdown and   $\\LaTeX$ in the space provided or by embedding an image of your hand written work.\n",
    "\n",
    "\n",
    "* __e) short response:__ Compare the classification you get from this unsmoothed model in `d`/`e` to the results in the textbook's \"Example 1\" which reflects a model with Laplace plus 1 smoothing. How does smoothing affect our inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __a)__ To calculate $\\hat{P}(c|d)$, we need  \n",
    ">  \n",
    "> (1) $\\hat{P}(c)$, the prior probablity of a document occuring in class c, and   \n",
    ">  \n",
    "> (2) $\\prod_{1 \\le k \\le n_d} \\hat{P}(t_k|c)$ where $P(t|c)$ is the conditional probability of term $t_k$ occurring in a document of class _**c**_. Since there could be hundreds or thousands of terms in each document, and in order to represent the document as class _**c**_ by all terms, we need to multiply the conditional probability of all terms.  \n",
    ">  \n",
    "> We used $\\hat{P}$ in both parameters because we do not know the true value of the parameters and will estimate from the training texts. \n",
    "\n",
    "\n",
    "> __b)__ Assuming __c__ as a spam email, the $\\hat{P}(c)$ would indicate the prior probability of the spam email. This can be calculated by  \n",
    "> $\\hat{P}(c) = \\frac{\\text{total number of spam emails}}{\\text{total number of emails = spam + ham emails}}$  \n",
    ">  \n",
    "> $\\hat{P}(t_k|c)$ is an estimated probability of a term $t_k$ given the email is a spam. This can be calculated by  \n",
    ">  \n",
    "> $\\hat{P}(t_k|c) = \\frac{\\text{total number of occurrence of term }t_k \\text{ in spam emails}}{\\text{total number of words in spam emails}}$  \n",
    ">   \n",
    "> These calculations can be done by one pass through the training data.  \n",
    "\n",
    "\n",
    "> __c)__ Show your calculations here using markdown & $\\LaTeX$ or embed them below!  \n",
    ">  \n",
    "> $\\hat{P}(c=1)$ = 3/4 = 0.75  \n",
    ">  \n",
    "> $\\hat{P}(c=0)$ = 1/4 = 0.25  \n",
    ">  \n",
    "> $\\hat{P}(Chinese|c=1)$ = 5/8  \n",
    ">  \n",
    "> $\\hat{P}(Chinese|c=0)$ = $\\hat{P}(Tokyo|c=0)$ = $\\hat{P}(Japan|c=0)$ =1/3  \n",
    ">   \n",
    "> $\\hat{P}(Beijing|c=1)$ = $\\hat{P}(Shanghai|c=1)$ = $\\hat{P}(Macao|c=1)$ = 1/8  \n",
    ">  \n",
    "> $\\hat{P}(Beijing|c=0)$ = $\\hat{P}(Shanghai|c=0)$ = $\\hat{P}(Macao|c=0)$ = 0  \n",
    ">  \n",
    "> $\\hat{P}(Tokyo|c=1)$ = $\\hat{P}(Japan|c=1)$ = 0\n",
    "\n",
    "\n",
    "\n",
    "> __d)__ Show your calculations here using markdown & $\\LaTeX$ or embed them below!  \n",
    ">  \n",
    "> test doc, **d** = `['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']`  \n",
    ">  \n",
    "> $\\hat{P}(c=1|d)$ = $\\hat{P}(c=1) \\cdot (5/8)^3 \\cdot 0 \\cdot 0  = 3/4 \\cdot 0 = 0$\n",
    ">  \n",
    "> $\\hat{P}(c=0|d)$ = $\\hat{P}(c=0) \\cdot (1/3)^3 \\cdot 1/3 \\cdot 1/3 = 1/4 \\cdot (1/3)^5 = 0.001$  \n",
    "\n",
    "\n",
    "> __e)__ Our calculations show that the document __d__ would have been classified as non-Chinese (c=0) because we don't have conditional probabilities for terms like Tokyo and Japan in Chinese documents, $\\hat{P}(Japan|c=1) = \\hat{P}(Tokyo|c=1) = 0$. Intuitively, the test document would have been **_Chinese_** with more occurrence of _Chinese_ words. Since our Naive Bayes model hasn't seen the terms such as Tokyo, Japan in the training set, the model has classified the document inaccurately. With Laplace smoothing, the zero conditional probabilities become non-zero and help the model to make a more accurate prediction.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part d/e - if you didn't write out your calcuations above, embed a picture of them here:\n",
    "from IPython.display import Image\n",
    "Image(filename=\"path-to-hand-calulations-image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Naive Bayes Inference.\n",
    "In the next two questions you'll write code to parallelize the Naive Bayes calculations that you performed above. We'll do this in two phases: one MapReduce job to perform training and a second MapReduce to perform inference. While in practice we'd need to train a model before we can use it to classify documents, for learning purposes we're going to develop our code in the opposite order. By first focusing on the pieces of information/format we need to perform the classification (inference) task you should find it easier to develop a solid implementation for training phase when you get to question 8 below. In both of these questions we'll continue to use the Chinese example corpus from the textbook to help us test our MapReduce code as we develop it. Below we've reproduced the corpus, test set and model in text format that matches the Enron data.\n",
    "\n",
    "### Q7 Tasks:\n",
    "* __a) short response:__ run the provided cells to create the example files and load them in to HDFS. Then take a closer look at __`NBmodel.txt`__. This text file represents a Naive Bayes model trained (with Laplace +1 smoothing) on the example corpus. What are the 'keys' and 'values' in this file? Which record means something slightly different than the rest? The value field of each record includes two numbers which will be helpful for debugging but which we don't actually need to perform inference -- what are they? [`HINT`: _This file represents the model from Example 13.1 in the textbook, if you're having trouble getting oriented try comparing our file to the numbers in that example._]\n",
    "\n",
    "\n",
    "* __b) short response:__ When performing Naive Bayes in practice instead of multiplying the probabilities (as in equation 13.3) we add their logs (as in equation 13.4). Why do we choose to work with log probabilities? If we had an unsmoothed model, what potential error could arise from this transformation?\n",
    "\n",
    "\n",
    "* __c) short response:__ Documents 6 and 8 in the test set include a word that did not appear in the training corpus (and as a result does not appear in the model). What should we do at inference time when we need a class conditional probability for this word?\n",
    "\n",
    "\n",
    "* __d) short response:__ The goal of our MapReduce job is to stream over the test set and classify each document by peforming the calculation from equation 13.4. To do this we'll load the model file (which contains the probabilities for equation 13.4) into memory on the nodes where we do our mapping. This is called an in-memory join. Does loading a model 'state' like this depart from the functional programming principles? Explain why or why not. From a scability perspective when would this kind of memory use be justified? when would it be unwise?\n",
    "\n",
    "\n",
    "* __e) code:__ Complete the code in __`NaiveBayes/classify_mapper.py`__. Read the docstring carefully to understand how this script should work and the format it should return. Run the provided unit tests to confirm that your script works as expected then write a Hadoop streaming job to classify the Chinese example test set. [`HINT 1:` _you shouldn't need a reducer for this one._ `HINT 2:` _Don't forget to add the model file to the_ `-files` _parameter in your Hadoop streaming job so that it gets shipped to the mapper nodes where it will be accessed by your script._]\n",
    "\n",
    "\n",
    "* __f) short response:__ In our test example and in the Enron data set we have fairly short documents. Since these fit fine in memory on a mapper node we didn't need a reducer and could just do all of our calculations in the mapper. However with much longer documents (eg. books) we might want a higher level of parallelization -- for example we might want to process parts of a document on different nodes. In this hypothetical scenario how would our algorithm design change? What could the mappers still do? What key-value structure would they emit? What would the reducers have to do as a last step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "> __a)__ In NBModel.txt, the keys are the word and the values are the fields after that, specifically  \n",
    "> 1. number of occurrence of the word in non-Chinese (c=0)  \n",
    "> 2. number of occurrence of the word in Chinese (c=1)  \n",
    "> 3. conditional probability of the word given the doc is non-Chinese (c=0)  \n",
    "> 4. conditional probability of the word given the doc is Chinese(c=1)  \n",
    ">  \n",
    "> Since the last two fields (3 and 4) are what we want in our calculation, the first fields (1 and 2) were dropped in classify_mapper.py.  \n",
    ">  \n",
    "> The record __ClassPriors__ is slightly different than the rest because the probabilities given are not conditional probabilities, but just a respective **Prior** probability, $\\hat{P}(c=0)$ and $\\hat{P}(c=1)$.  \n",
    "\n",
    "> __b)__ Taking log on maximum likelihood (multiplication of probabilities) turns an underlying multiplication into summation of log probabilities, eg., `log(xy) = log(x) + log(y)`. By doing so also removes the problem of floating point underflow by successive multiplications. However, since we're taking the log probability, for an unsmoothened model, taking logarithm, however, can cause an error if the word occurrence in a certain class is **0** because log(0) is undefined.  \n",
    "\n",
    "> __c)__ Documents 6 and 8 contain the word `Trade` that was never in the training documents. As a result, the conditional probability of the word `Trade` is **0** which will be used accordingly in inference time.  \n",
    "\n",
    "> __d)__ Functional programming principles is based on evaluation of mathematical operation, the result of which should not affect or should not be affected by other operations. For example, the word __'Chinese'__ count in `['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']` is 3 no matter what other operations take place in other places. When we use a model which comes with corresponding weights or conditional probabilities in this scenario, this is a slight departure from one of the functional programming principles because the model can change the output of our operation. The model itself can be changed, depending on the number of documents we'd train. Total number of word occurrence changes as we change our documents, and the conditional probability changes as a result.  \n",
    ">  \n",
    "> This departure is justified when we can control the operation of the model, eg, if the information in memory is the same across all nodes (hadoop datanodes). It would be unwise if the operation on one node will produce a result which would then change other operational results from other nodes. \n",
    "\n",
    "> __e)__ Complete the coding portion of this question before answering 'f'.\n",
    "\n",
    "> __f)__ For classifying larger documents (eg. books), we might want a higher level of parallelization with multiple mappers across multiple nodes. In this situation, we cannot calculate the predicted class from each mapper. The reducer will receive the emitted output from each mapper in the record format (doc(id), partial sum of logpHam, partial sum of logpSpam). The aggregration of log probability for each class (Ham or Spam) will be done in the reducer with respective to document ids. We can then predict the document or text or email by taking the exponential of the aggregated log probability for each document and predict their class.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to create the example corpus and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTrain.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTrain.txt\n",
    "D1\t1\t\tChinese Beijing Chinese\n",
    "D2\t1\t\tChinese Chinese Shanghai\n",
    "D3\t1\t\tChinese Macao\n",
    "D4\t0\t\tTokyo Japan Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NaiveBayes/chineseTest.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NaiveBayes/chineseTest.txt\n",
    "D5\t1\t\tChinese Chinese Chinese Tokyo Japan\n",
    "D6\t1\t\tBeijing Shanghai Trade\n",
    "D7\t0\t\tJapan Macao Tokyo\n",
    "D8\t0\t\tTokyo Japan Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting NBmodel.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile NBmodel.txt\n",
    "beijing\t0.0,1.0,0.111111111111,0.142857142857\n",
    "chinese\t1.0,5.0,0.222222222222,0.428571428571\n",
    "tokyo\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "shanghai\t0.0,1.0,0.111111111111,0.142857142857\n",
    "ClassPriors\t1.0,3.0,0.25,0.75\n",
    "japan\t1.0,0.0,0.222222222222,0.0714285714286\n",
    "macao\t0.0,1.0,0.111111111111,0.142857142857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/chineseTrain.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/chineseTest.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# load the data files into HDFS\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTrain.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/chineseTest.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work for `part e` starts here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - do your work in NaiveBayes/classify_mapper.py first, then run this cell.\n",
    "!chmod a+x NaiveBayes/classify_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - unit test NaiveBayes/classify_mapper.py (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear the output directory in HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob8870566797316105413.jar tmpDir=null\n",
      "19/01/16 23:41:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:41:29 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:41:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:41:30 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:41:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0036\n",
      "19/01/16 23:41:31 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0036\n",
      "19/01/16 23:41:31 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0036/\n",
      "19/01/16 23:41:31 INFO mapreduce.Job: Running job: job_1547262255608_0036\n",
      "19/01/16 23:41:39 INFO mapreduce.Job: Job job_1547262255608_0036 running in uber mode : false\n",
      "19/01/16 23:41:39 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:41:47 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/16 23:41:48 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:41:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:41:55 INFO mapreduce.Job: Job job_1547262255608_0036 completed successfully\n",
      "19/01/16 23:41:55 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=192\n",
      "\t\tFILE: Number of bytes written=448793\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=403\n",
      "\t\tHDFS: Number of bytes written=178\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12401\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4638\n",
      "\t\tTotal time spent by all map tasks (ms)=12401\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4638\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12401\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4638\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12698624\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4749312\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=4\n",
      "\t\tMap output bytes=178\n",
      "\t\tMap output materialized bytes=198\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=198\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=8\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=136\n",
      "\t\tCPU time spent (ms)=2230\n",
      "\t\tPhysical memory (bytes) snapshot=764137472\n",
      "\t\tVirtual memory (bytes) snapshot=4116983808\n",
      "\t\tTotal committed heap usage (bytes)=681574400\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=177\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=178\n",
      "19/01/16 23:41:55 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadooop streaming job here\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NBmodel.txt,NaiveBayes/classify_mapper.py \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/chineseTest.txt \\\n",
    "  -output {HDFS_DIR}/chinese-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - retrieve test set results from HDFS (RUN THIS CELL AS IS)\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-output/part-000* > NaiveBayes/chineseResults.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5  1  -8.90668134500626   -8.10769031284611   1\n",
      "d6  1  -5.780743515794329  -4.179502370564408  1\n",
      "d7  0  -6.591673732011658  -7.511706880737812  0\n",
      "d8  0  -4.394449154674438  -5.565796731681498  0\n"
     ]
    }
   ],
   "source": [
    "# part e - take a look (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseResults.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th> Expected output for the test set:</th>\n",
    "<tr align=Left><td><pre>\n",
    "d5\t1\t-8.90668134\t-8.10769031\t1\n",
    "d6\t1\t-5.78074351\t-4.17950237\t1\n",
    "d7\t0\t-6.59167373\t-7.51170688\t0\n",
    "d8\t0\t-4.39444915\t-5.56579673\t0\n",
    "</pre></td><tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Naive Bayes Training.\n",
    "In Question 7 we used a model that we had trained by hand. Next we'll develop the code to do that same training in parallel, making it suitable for use with larger corpora (like the Enron emails). The end result of the MapReduce job you write in this question should be a model text file that looks just like the example (`NBmodel.txt`) that we created by hand above.\n",
    "\n",
    "To refresh your memory about the training process take a look at  `6a` and `6b` where you described the pieces of information you'll need to collect in order to encode a Multinomial Naive Bayes model. We now want to retrieve those pieces of information while streaming over a corpus. The bulk of the task will be very similar to the word counting excercises you've already done but you may want to consider a slightly different key-value record structure to efficiently tally counts for each class. \n",
    "\n",
    "The most challenging (interesting?) design question will be how to retrieve the totals (# of documents and # of words in documents for each class). Of course, counting these numbers is easy. The hard part is the timing: you'll need to make sure you have the counts totalled up _before_ you start estimating the class conditional probabilities for each word. It would be best (i.e. most scalable) if we could find a way to do this tallying without storing the whole vocabulary in memory... Use an appropriate MapReduce design pattern to implement this efficiently! \n",
    "\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) make a plan:__  Fill in the docstrings for __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ to appropriately reflect the format that each script will input/output. [`HINT:` _the input files_ (`enronemail_1h.txt` & `chineseTrain.txt`) _have a prespecified format and your output file should match_ `NBmodel.txt` _so you really only have to decide on an internal format for Hadoop_].\n",
    "\n",
    "\n",
    "* __b) implement it:__ Complete the code in __`NaiveBayes/train_mapper.py`__ and __`NaiveBayes/train_reducer.py`__ so that together they train a Multinomial Naive Bayes model __with no smoothing__. Make sure your end result is formatted correctly (see note above). Test your scripts independently and together (using `chineseTrain.txt` or test input of your own devising). When you are satisfied with your Python code design and run a Hadoop streaming command to run your job in parallel on the __chineseTrain.txt__. Confirm that your trained model matches your hand calculations from Question 5.\n",
    "\n",
    "\n",
    "* __c) short response:__ We saw in Question 6 that adding Laplace +1 smoothing makes our classifications less sensitve to rare words. However implementing this technique requires access to one additional piece of information that we had not previously used in our Naive Bayes training. What is that extra piece of information? [`HINT:` see equation 13.7 in Manning, Raghavan and Schutze].\n",
    "\n",
    "\n",
    "* __d) short response:__ There are three approaches that we could take to handle the extra piece of information you identified in `c`: 1) we could hard code it into our reducer (_where would we get it in the first place?_). Or 2) we could compute it inside the reducer which would require storing some information in memory (_what information?_). Or 3) we could compute it in the reducer without storing any bulky information in memory but then we'd need some postprocessing or a second MapReduce job to complete the calculation (_why?_). Breifly explain what is non-ideal about each of these options. BONUS: which of these options is incompatible with using multiple reducers?\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Choose one of the 3 options above. State your choice & reasoning in the space below then use that strategy to complete the code in __`NaiveBayes/train_reducer_smooth.py`__. Test this alternate reducer then write and run a Hadoop streaming job to train an MNB model with smoothing on the Chinese example. Your results should match the model that we provided for you above (and the calculations in the textbook example). [`HINT:` _don't start from scratch with this one -- you can just copy over your reducer code from part `b` and make the needed modifications_]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "> __ c)__ We need a total number of unique words in the entire documents. \n",
    "\n",
    "> __ d)__  \n",
    ">  \n",
    "__1. Hardcoded approach__  \n",
    "We could use the **set** method to count the total number of unique words in the entire documents in the reducer. We could hard code the function in the reducer to load the entire document and count the unique words. This function will be separate from a regular reducer function. However this would require a large chunk of memory if the document is large. In addition, since we're loading from the raw document, it would also require to preprocess the documents so that the total number of unique words we're counting will reflect all the words coming from the mapper. The other source we could also get the vocabulary count is from the mapper and emit in the record. In that case, mapper would also require to preprocess the entire document rather than reading line by line. This approach allows to use multiple reducers, however it falls short of providing the better memory management. \n",
    ">  \n",
    "> __2. Dictionary approach__  \n",
    "The other approach is we can record the number of words in a dictionary as they are received in the reducer. However, we would need to wait until all the records from the mapper are done emitting because another unique word could appear in the last emitted record. We would store information on words and their respective count on each of the labels. Eg, 'beijing' word will be recorded with its total number of occurrence in each labels; c=0 and c=1. This approach is not scalable because if there are multiple reducers, they need to communicate with each other in order to calculate the true total number of unique words. However, for a smaller dataset, this approach reassures that there won't be any redundant steps where additional mistakes can be introduced (eg., the hard-coded approach with preprocessing step).  \n",
    ">  \n",
    "> __3. Counter approach__  \n",
    "> We can also employ counter approach where we just count the unique words in each reducer. This removes the necessity of the large memory usage because we are storing only the counter. This approach is also scalable because we can deploy as many reducers as we want and each reducer will store their respective counters for unique words. However, because of this, we'd need an extra MapReduce step where counter from each reducer is aggregated and conditional probability of each word with laplace smoothing can be calculated in the final step.  \n",
    "> \n",
    "> __ e)__ I chose the dictionary approach (2nd) because of the small dataset we're processing. This also removes the extra steps of pre-processing and post-processing. I created the vocabulary dictionary with list as a value for each word so that I can store information for total number of word occurrence in each label; c=0 and c=1. However this approach is not scalable due to the nature of storing unique corpus in dictionary format in memory. If the entire corpus is huge, I would go with the 3rd approach (counter) and employ multiple reducers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== MAPPER DOCSTRING ============\n",
      "Mapper reads in text documents and emits word counts by class.\n",
      "\n",
      "INPUT:\n",
      "    ID \\t true_class \\t subject \\t body \\n\n",
      "OUTPUT:\n",
      "    word \\t true_class \\t count\n",
      "=========== REDUCER DOCSTRING ============\n",
      "Reducer aggregates word counts by class and emits frequencies.\n",
      "\n",
      "INPUT:\n",
      "    word \\t true_class \\t count\n",
      "OUTPUT:\n",
      "    word \\t count_Ham \\t count_Spam \\t cp_Ham \\t cp_Spam\n"
     ]
    }
   ],
   "source": [
    "# part a - do your work in train_mapper.py and train_reducer.py then RUN THIS CELL AS IS\n",
    "!chmod a+x NaiveBayes/train_mapper.py\n",
    "!chmod a+x NaiveBayes/train_reducer.py\n",
    "!echo \"=========== MAPPER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_mapper.py | tail -n 6\n",
    "!echo \"=========== REDUCER DOCSTRING ============\"\n",
    "!head -n 8 NaiveBayes/train_reducer.py | tail -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part b starts here`:__ MNB _without_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*         0  1\n",
      "*         1  1\n",
      "*         1  1\n",
      "*         1  1\n",
      ".         0  1\n",
      ".         0  1\n",
      ".         0  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      ".         1  1\n",
      "beijing   1  1\n",
      "chinese   0  1\n",
      "chinese   1  1\n",
      "chinese   1  1\n",
      "chinese   1  1\n",
      "chinese   1  1\n",
      "chinese   1  1\n",
      "japan     0  1\n",
      "macao     1  1\n",
      "shanghai  1  1\n",
      "tokyo     0  1\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your mapper here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.0,0.125\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "tokyo        1,0,0.3333333333333333,0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - write a unit test for your reducer here\n",
    "# Creating a custom input for reducer \n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 | column -t > NaiveBayes/train_reducer_test_input.txt\n",
    "\n",
    "#test reducer\n",
    "!cat NaiveBayes/train_reducer_test_input.txt | NaiveBayes/train_reducer.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.0,0.125\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "tokyo        1,0,0.3333333333333333,0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 -k2,2n | NaiveBayes/train_reducer.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part b - clear (and name) an output directory in HDFS for your unsmoothed chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-unsmooth-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7217577082093140659.jar tmpDir=null\n",
      "19/01/16 23:42:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:42:38 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:42:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:42:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:42:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0037\n",
      "19/01/16 23:42:39 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0037\n",
      "19/01/16 23:42:39 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0037/\n",
      "19/01/16 23:42:39 INFO mapreduce.Job: Running job: job_1547262255608_0037\n",
      "19/01/16 23:42:49 INFO mapreduce.Job: Job job_1547262255608_0037 running in uber mode : false\n",
      "19/01/16 23:42:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:42:59 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/16 23:43:00 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:43:08 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:43:08 INFO mapreduce.Job: Job job_1547262255608_0037 completed successfully\n",
      "19/01/16 23:43:08 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275\n",
      "\t\tFILE: Number of bytes written=450753\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=387\n",
      "\t\tHDFS: Number of bytes written=194\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=15401\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4556\n",
      "\t\tTotal time spent by all map tasks (ms)=15401\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4556\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=15401\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4556\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15770624\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4665344\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=217\n",
      "\t\tMap output materialized bytes=281\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=281\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=52\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=90\n",
      "\t\tCPU time spent (ms)=2300\n",
      "\t\tPhysical memory (bytes) snapshot=736440320\n",
      "\t\tVirtual memory (bytes) snapshot=4129652736\n",
      "\t\tTotal committed heap usage (bytes)=518520832\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=159\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=194\n",
      "19/01/16 23:43:08 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part b - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-unsmooth-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part b - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-unsmooth-output/part-000* > NaiveBayes/chinese_unsmooth_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.0,0.125\n",
      "chinese      1,5,0.3333333333333333,0.625\n",
      "japan        1,0,0.3333333333333333,0.0\n",
      "macao        0,1,0.0,0.125\n",
      "shanghai     0,1,0.0,0.125\n",
      "tokyo        1,0,0.3333333333333333,0.0\n"
     ]
    }
   ],
   "source": [
    "# part b - print your model so that we can confirm that it matches expected results\n",
    "!cat NaiveBayes/chinese_unsmooth_results.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`part e starts here`:__ MNB _with_ Smoothing (training on Chinese Example Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.1111111111111111,0.14285714285714285\n",
      "chinese      1,5,0.2222222222222222,0.42857142857142855\n",
      "japan        1,0,0.2222222222222222,0.07142857142857142\n",
      "macao        0,1,0.1111111111111111,0.14285714285714285\n",
      "shanghai     0,1,0.1111111111111111,0.14285714285714285\n",
      "tokyo        1,0,0.2222222222222222,0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "# part e - write a unit test for your NEW reducer here\n",
    "!chmod a+x NaiveBayes/train_reducer_smooth.py\n",
    "!cat NaiveBayes/train_reducer_test_input.txt | NaiveBayes/train_reducer_smooth.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.1111111111111111,0.14285714285714285\n",
      "chinese      1,5,0.2222222222222222,0.42857142857142855\n",
      "japan        1,0,0.2222222222222222,0.07142857142857142\n",
      "macao        0,1,0.1111111111111111,0.14285714285714285\n",
      "shanghai     0,1,0.1111111111111111,0.14285714285714285\n",
      "tokyo        1,0,0.2222222222222222,0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "# part e - write a systems test for your mapper + reducer together here\n",
    "!cat NaiveBayes/chineseTrain.txt | NaiveBayes/train_mapper.py | sort -k1,1 | NaiveBayes/train_reducer_smooth.py | column -t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/chinese-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part e - clear (and name) an output directory in HDFS for your SMOOTHED chinese NB model\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/chinese-smooth-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob7476726953767136956.jar tmpDir=null\n",
      "19/01/16 23:44:23 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:44:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:44:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:44:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:44:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0038\n",
      "19/01/16 23:44:25 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0038\n",
      "19/01/16 23:44:25 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0038/\n",
      "19/01/16 23:44:25 INFO mapreduce.Job: Running job: job_1547262255608_0038\n",
      "19/01/16 23:44:34 INFO mapreduce.Job: Job job_1547262255608_0038 running in uber mode : false\n",
      "19/01/16 23:44:34 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:44:43 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/16 23:44:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:44:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:44:51 INFO mapreduce.Job: Job job_1547262255608_0038 completed successfully\n",
      "19/01/16 23:44:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=275\n",
      "\t\tFILE: Number of bytes written=450852\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=387\n",
      "\t\tHDFS: Number of bytes written=327\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13882\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5015\n",
      "\t\tTotal time spent by all map tasks (ms)=13882\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5015\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13882\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5015\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14215168\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5135360\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=217\n",
      "\t\tMap output materialized bytes=281\n",
      "\t\tInput split bytes=228\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=281\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=52\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=103\n",
      "\t\tCPU time spent (ms)=2390\n",
      "\t\tPhysical memory (bytes) snapshot=743411712\n",
      "\t\tVirtual memory (bytes) snapshot=4119986176\n",
      "\t\tTotal committed heap usage (bytes)=520617984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=159\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=327\n",
      "19/01/16 23:44:51 INFO streaming.StreamJob: Output directory: /user/root/HW2/chinese-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your hadoop streaming job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/chineseTrain.txt \\\n",
    "  -output {HDFS_DIR}/chinese-smooth-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part e - extract your results (i.e. model) to a local file\n",
    "!hdfs dfs -cat {HDFS_DIR}/chinese-smooth-output/part-000* > NaiveBayes/chinese_smooth_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassPriors  1,3,0.25,0.75\n",
      "beijing      0,1,0.1111111111111111,0.14285714285714285\n",
      "chinese      1,5,0.2222222222222222,0.42857142857142855\n",
      "japan        1,0,0.2222222222222222,0.07142857142857142\n",
      "macao        0,1,0.1111111111111111,0.14285714285714285\n",
      "shanghai     0,1,0.1111111111111111,0.14285714285714285\n",
      "tokyo        1,0,0.2222222222222222,0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "!cat NaiveBayes/chinese_smooth_results.txt | column -t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Enron Ham/Spam NB Classifier & Results.\n",
    "\n",
    "Fantastic work. We're finally ready to perform Spam Classification on the Enron Corpus. In this question you'll run the analysis you've developed, report its performance, and draw some conclusions.\n",
    "\n",
    "### Q9 Tasks:\n",
    "* __a) train/test split:__ Run the provided code to split our Enron file into a training set and testing set then load them into HDFS. \n",
    "\n",
    "[`NOTE:` _If you hard coded the vocab size in question 8d make sure you re calculate the vocab size for just the training set!_]\n",
    "\n",
    "* __b) train 2 models:__ Write Hadoop Streaming jobs to train MNB Models on the training set with and without smoothing. Save your models to local files at __`NaiveBayes/Unsmoothed/NBmodel.txt`__ and __`NaiveBayes/Smoothed/NBmodel.txt`__. [`NOTE:` _This naming is important because we wrote our classification task so that it expects a file of that name... if this inelegance frustrates you there is an alternative that would involve a few adjustments to your code [read more about it here](http://www.tnoda.com/blog/2013-11-23)._] Finally run the checks that we provide to confirm that your results are correct.\n",
    "\n",
    "\n",
    "* __c) code:__ Recall that we designed our classification job with just a mapper. An efficient way to report the performance of our models would be to simply add a reducer phase to this job and compute precision and recall right there. Complete the code in __`NaiveBayes/evaluation_reducer.py`__ and then write Hadoop jobs to evaluate your two models on the test set. Report their performance side by side. [`NOTE:` if you need a refresher on precision, recall and F1-score [Wikipedia](https://en.wikipedia.org/wiki/F1_score) is a good resource.]\n",
    "\n",
    "\n",
    "* __d) short response:__ Compare the performance of your two models. What do you notice about the unsmoothed model's predictions? Can you guess why this is happening? Which evaluation measure do you think is most relevant in our use case? [`NOTE:` _Feel free to answer using your common sense but if you want more information on evaluating the classification task checkout_ [this blogpost](https://tryolabs.com/blog/2013/03/25/why-accuracy-alone-bad-measure-classification-tasks-and-what-we-can-do-about-it/\n",
    ") or [this paper](http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf\n",
    ")]\n",
    "\n",
    "\n",
    "* __e) code + short response:__ Let's look at the top ten words with the highest conditional probability in `Spam` and in `Ham`. We'll do this by writing a Hadoop job that sorts the model file (`NaiveBayes/Smoothed/NBmodel.py`). Normally we'd have to run two jobs -- one that sorts on $P(word|ham)$ and another that sorts on $P(word|spam)$. However if we slighly modify the data format in the model file then we can get the top words in each class with just one job. We've written a mapper that will do just this for you. Read through __`NaiveBayes/model_sort_mapper.py`__ and then briefly explain how this mapper will allow us to partition and sort our model file. Write a Hadoop job that uses our mapper and `/bin/cat` for a reducer to partition and sort. Print out the top 10 words in each class (where 'top' == highest conditional probability).[`HINT:` _this should remind you a lot of what we did in Question 6._]\n",
    "\n",
    "\n",
    "* __f) short response:__ What do you notice about the 'top words' we printed in `e`? How would increasing the smoothing parameter 'k' affect the probabilities for the top words that you identified for 'e'. How would they affect the probabilities of words that occur much more in one class than another? In summary, how does the smoothing parameter 'k' affect the bias and the variance of our model. [`NOTE:` _you do not need to code anything for this task, but if you are struggling with it you could try changing 'k' and see what happens to the test set. We don't recommend doing this exploration with the Enron data because it will be harder to see the impact with such a big vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 Student Answers:\n",
    "> __d)__ The unsmoothened model has very poor classification and there was only 1 true positive out of 20 documents classified. What is also interesting is the unsmoothened model has far more better prediction on true negatives: **9** out of 20 than its counterpart smoothened model, __6__ out of 20 documents. This result underlines in the mechanism of Naive Bayes conditional probability calculation. Since unseen words are counted as zero probablity in a given classification (c=0 or c=1), when taking logs, it becomes $-\\infty$ and the document will be labeled as *negative* even if it has many positive words which were never seen in the corresponding training documents. \n",
    "\n",
    "> __e)__ The **model_sort_mapper.py** mapper checks every word and their associated conditional probabilities for both classification (c=0 and c=1). Comparing between the two, taking the higher probability, the mapper classifies the word accordingly into either Ham or Spam category. The emitted records are sorted in /bin/cat. \n",
    "\n",
    "> __f)__ The top 10 words between each classification do not have any insightful information that we can use to classify the email as either Ham or Spam. Majority of words are English common words such as particles: \"to\", \"a\", \"the\" or pronouns: \"you\", \"your\". Although each word stands on its own as highest conditional probability for Ham or Spam classification, it would be very hard to classify emails based on those few words. I suspect the composite of those words in the entire email is what makes it classification possible with their respective conditional probabilities. Increasing the smoothing parameter **k** would affect the probabilities of those words. Since each word conditional probability is $\\hat{P}(t_k|c)$ for Ham and $\\hat{P}(t_k|\\bar{c})$ for Spam, by increasing the parameter k, we're artificially increasing the occurrence of the word in a given document and making the model more generalized, i.e., words are artificially distributed across all documents even if they don't represent or occur in a document. So choosing the right parameter k is crucial to balance between bias and variance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test/Train split__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `/user/root/HW2/enron_train.txt': File exists\n",
      "copyFromLocal: `/user/root/HW2/enron_test.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# part a - test/train split (RUN THIS CELL AS IS)\n",
    "!head -n 80 data/enronemail_1h.txt > data/enron_train.txt\n",
    "!tail -n 20 data/enronemail_1h.txt > data/enron_test.txt\n",
    "!hdfs dfs -copyFromLocal data/enron_train.txt {HDFS_DIR}\n",
    "!hdfs dfs -copyFromLocal data/enron_test.txt {HDFS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _without smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob4052735845300804375.jar tmpDir=null\n",
      "19/01/16 23:45:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:45:24 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/16 23:45:25 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/16 23:45:25 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/16 23:45:25 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0039\n",
      "19/01/16 23:45:26 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0039\n",
      "19/01/16 23:45:26 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0039/\n",
      "19/01/16 23:45:26 INFO mapreduce.Job: Running job: job_1547262255608_0039\n",
      "19/01/16 23:45:33 INFO mapreduce.Job: Job job_1547262255608_0039 running in uber mode : false\n",
      "19/01/16 23:45:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/16 23:45:42 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/16 23:45:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/16 23:45:49 INFO mapreduce.Job: Job job_1547262255608_0039 completed successfully\n",
      "19/01/16 23:45:49 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=494510\n",
      "\t\tFILE: Number of bytes written=1439184\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=186923\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12364\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4509\n",
      "\t\tTotal time spent by all map tasks (ms)=12364\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4509\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12364\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4509\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=12660736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4617216\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=50214\n",
      "\t\tMap output bytes=394076\n",
      "\t\tMap output materialized bytes=494516\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5422\n",
      "\t\tReduce shuffle bytes=494516\n",
      "\t\tReduce input records=50214\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=100428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=76\n",
      "\t\tCPU time spent (ms)=3950\n",
      "\t\tPhysical memory (bytes) snapshot=779046912\n",
      "\t\tVirtual memory (bytes) snapshot=4110749696\n",
      "\t\tTotal committed heap usage (bytes)=624427008\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=186923\n",
      "19/01/16 23:45:49 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-model\n",
      "mkdir: cannot create directory `NaiveBayes/Unsmoothed': File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Unsmoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/enron-model \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!mkdir NaiveBayes/Unsmoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-model/part-000* > NaiveBayes/Unsmoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001725476662928134,0.00029682398337785694\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000172547666293,0.000296823983378"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,8.62738331464067e-05,0.001632531908578213\n"
     ]
    }
   ],
   "source": [
    "# part b - check your UNSMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Unsmoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,8.62738331464e-05,0.00163253190858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training__ (Enron MNB Model _with Laplace +1 smoothing_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/smooth-model\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob6338038141466930427.jar tmpDir=null\n",
      "19/01/17 00:00:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:00:34 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:00:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/17 00:00:35 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/17 00:00:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0040\n",
      "19/01/17 00:00:36 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0040\n",
      "19/01/17 00:00:36 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0040/\n",
      "19/01/17 00:00:36 INFO mapreduce.Job: Running job: job_1547262255608_0040\n",
      "19/01/17 00:00:44 INFO mapreduce.Job: Job job_1547262255608_0040 running in uber mode : false\n",
      "19/01/17 00:00:44 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/17 00:00:52 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/17 00:00:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/17 00:01:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/17 00:01:06 INFO mapreduce.Job: Job job_1547262255608_0040 completed successfully\n",
      "19/01/17 00:01:06 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=494510\n",
      "\t\tFILE: Number of bytes written=1439292\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=167388\n",
      "\t\tHDFS: Number of bytes written=254172\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13188\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11202\n",
      "\t\tTotal time spent by all map tasks (ms)=13188\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11202\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13188\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11202\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13504512\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11470848\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=80\n",
      "\t\tMap output records=50214\n",
      "\t\tMap output bytes=394076\n",
      "\t\tMap output materialized bytes=494516\n",
      "\t\tInput split bytes=226\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5422\n",
      "\t\tReduce shuffle bytes=494516\n",
      "\t\tReduce input records=50214\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=100428\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tCPU time spent (ms)=4280\n",
      "\t\tPhysical memory (bytes) snapshot=777138176\n",
      "\t\tVirtual memory (bytes) snapshot=4116443136\n",
      "\t\tTotal committed heap usage (bytes)=623378432\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=167162\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=254172\n",
      "19/01/17 00:01:06 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-model\n",
      "mkdir: cannot create directory `NaiveBayes/Smoothed': File exists\n"
     ]
    }
   ],
   "source": [
    "# part b -  Smoothed model (FILL IN THE MISSING CODE BELOW)\n",
    "\n",
    "# clear the output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-model\n",
    "\n",
    "# hadoop command\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=2 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2n\" \\\n",
    "  -files NaiveBayes/train_mapper.py,NaiveBayes/train_reducer_smooth.py \\\n",
    "  -mapper train_mapper.py \\\n",
    "  -reducer train_reducer_smooth.py \\\n",
    "  -input {HDFS_DIR}/enron_train.txt \\\n",
    "  -output {HDFS_DIR}/smooth-model \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# save the model locally\n",
    "!mkdir NaiveBayes/Smoothed\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-model/part-000* > NaiveBayes/Smoothed/NBmodel.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t2,4,0.0001858045336306206,0.00027730020520215184\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep assistance NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: assistance\t2,4,0.000185804533631,0.000277300205202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "money\t1,22,0.0001238696890870804,0.0012755809439298986\n"
     ]
    }
   ],
   "source": [
    "# part b - check your SMOOTHED model results (RUN THIS CELL AS IS)\n",
    "!grep money NaiveBayes/Smoothed/NBmodel.txt\n",
    "# EXPECTED OUTPUT: money\t1,22,0.000123869689087,0.00127558094393"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part c - write your code in NaiveBayes/evaluation_reducer.py then RUN THIS\n",
    "!chmod a+x NaiveBayes/evaluation_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t1\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t1\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t0\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t0\n",
      "d5\t1\t-8.90668134500626\t-8.10769031284611\t True\n",
      "d6\t1\t-5.780743515794329\t-4.179502370564408\t True\n",
      "d7\t0\t-6.591673732011658\t-7.511706880737812\t True\n",
      "d8\t0\t-4.394449154674438\t-5.565796731681498\t True\n",
      "# Documents:      4.0\n",
      "True Positives:   2.0\n",
      "True Negatives:   2.0\n",
      "False Positives:  0.0\n",
      "False Negatives:  0.0\n",
      "Accuracy          1.0\n",
      "Precision         1.0\n",
      "Recall            1.0\n",
      "F-score           1.0\n"
     ]
    }
   ],
   "source": [
    "# part c - unit test your evaluation job on the chinese model (RUN THIS CELL AS IS)\n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py \n",
    "!cat NaiveBayes/chineseTest.txt | NaiveBayes/classify_mapper.py | NaiveBayes/evaluation_reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-unsmooth-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob3951322220624242896.jar tmpDir=null\n",
      "19/01/17 00:01:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:01:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:01:38 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/17 00:01:38 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/17 00:01:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0041\n",
      "19/01/17 00:01:39 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0041\n",
      "19/01/17 00:01:39 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0041/\n",
      "19/01/17 00:01:39 INFO mapreduce.Job: Running job: job_1547262255608_0041\n",
      "19/01/17 00:01:48 INFO mapreduce.Job: Job job_1547262255608_0041 running in uber mode : false\n",
      "19/01/17 00:01:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/17 00:01:57 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/17 00:01:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/17 00:02:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/17 00:02:05 INFO mapreduce.Job: Job job_1547262255608_0041 completed successfully\n",
      "19/01/17 00:02:05 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=818\n",
      "\t\tFILE: Number of bytes written=451284\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1103\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=13004\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5335\n",
      "\t\tTotal time spent by all map tasks (ms)=13004\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5335\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=13004\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5335\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13316096\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5463040\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=772\n",
      "\t\tMap output materialized bytes=824\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=824\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=121\n",
      "\t\tCPU time spent (ms)=2530\n",
      "\t\tPhysical memory (bytes) snapshot=793767936\n",
      "\t\tVirtual memory (bytes) snapshot=4114952192\n",
      "\t\tTotal committed heap usage (bytes)=625999872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1103\n",
      "19/01/17 00:02:05 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-unsmooth-output\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the UNSMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-unsmooth-output\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Unsmoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/enron-unsmooth-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-unsmooth-output/part-000* > NaiveBayes/Unsmoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/HW2/enron-smooth-output\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob4904108726057978050.jar tmpDir=null\n",
      "19/01/17 00:02:20 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:02:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:02:22 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/17 00:02:22 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/17 00:02:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0042\n",
      "19/01/17 00:02:22 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0042\n",
      "19/01/17 00:02:23 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0042/\n",
      "19/01/17 00:02:23 INFO mapreduce.Job: Running job: job_1547262255608_0042\n",
      "19/01/17 00:02:32 INFO mapreduce.Job: Job job_1547262255608_0042 running in uber mode : false\n",
      "19/01/17 00:02:32 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/17 00:02:42 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/17 00:02:43 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/17 00:02:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/17 00:02:51 INFO mapreduce.Job: Job job_1547262255608_0042 completed successfully\n",
      "19/01/17 00:02:51 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1349\n",
      "\t\tFILE: Number of bytes written=452334\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=49909\n",
      "\t\tHDFS: Number of bytes written=1612\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14391\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5688\n",
      "\t\tTotal time spent by all map tasks (ms)=14391\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5688\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14391\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=5688\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14736384\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=5824512\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=20\n",
      "\t\tMap output records=20\n",
      "\t\tMap output bytes=1303\n",
      "\t\tMap output materialized bytes=1355\n",
      "\t\tInput split bytes=224\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=1355\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=29\n",
      "\t\tSpilled Records=40\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=115\n",
      "\t\tCPU time spent (ms)=2580\n",
      "\t\tPhysical memory (bytes) snapshot=731594752\n",
      "\t\tVirtual memory (bytes) snapshot=4129366016\n",
      "\t\tTotal committed heap usage (bytes)=520617984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49685\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1612\n",
      "19/01/17 00:02:51 INFO streaming.StreamJob: Output directory: /user/root/HW2/enron-smooth-output\n"
     ]
    }
   ],
   "source": [
    "# part c - Evaluate the SMOOTHED Model Here (FILL IN THE MISSING CODE)\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/enron-smooth-output\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -files NaiveBayes/classify_mapper.py,NaiveBayes/evaluation_reducer.py,NaiveBayes/Smoothed/NBmodel.txt \\\n",
    "  -mapper classify_mapper.py \\\n",
    "  -reducer evaluation_reducer.py \\\n",
    "  -input {HDFS_DIR}/enron_test.txt \\\n",
    "  -output {HDFS_DIR}/enron-smooth-output \\\n",
    "  -numReduceTasks 1 \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/enron-smooth-output/part-000* > NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== UNSMOOTHED MODEL ============\n",
      "# Documents:      20.0\t\n",
      "True Positives:   1.0\t\n",
      "True Negatives:   9.0\t\n",
      "False Positives:  0.0\t\n",
      "False Negatives:  10.0\t\n",
      "Accuracy          0.5\t\n",
      "Precision         1.0\t\n",
      "Recall            0.09090909090909091\t\n",
      "F-score           0.16666666666666669\t\n",
      "=========== SMOOTHED MODEL ============\n",
      "# Documents:      20.0\t\n",
      "True Positives:   11.0\t\n",
      "True Negatives:   6.0\t\n",
      "False Positives:  3.0\t\n",
      "False Negatives:  0.0\t\n",
      "Accuracy          0.85\t\n",
      "Precision         0.7857142857142857\t\n",
      "Recall            1.0\t\n",
      "F-score           0.88\t\n"
     ]
    }
   ],
   "source": [
    "# part c - display results \n",
    "# NOTE: feel free to modify the tail commands to match the format of your results file\n",
    "print('=========== UNSMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Unsmoothed/results.txt\n",
    "print('=========== SMOOTHED MODEL ============')\n",
    "!tail -n 9 NaiveBayes/Smoothed/results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`EXPECTED RESULTS:`__ \n",
    "<table>\n",
    "<th>Unsmoothed Model</th>\n",
    "<th>Smoothed Model</th>\n",
    "<tr>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t1\n",
    "True Negatives:\t9\n",
    "False Positives:\t0\n",
    "False Negatives:\t10\n",
    "Accuracy\t0.5\n",
    "Precision\t1.0\n",
    "Recall\t0.0909\n",
    "F-Score\t0.1666\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "# Documents:\t20\n",
    "True Positives:\t11\n",
    "True Negatives:\t6\n",
    "False Positives:\t3\n",
    "False Negatives:\t0\n",
    "Accuracy\t0.85\n",
    "Precision\t0.7857\n",
    "Recall\t1.0\n",
    "F-Score\t0.88\n",
    "</pre></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "__`NOTE:`__ _Don't be too disappointed if these seem low to you. We've trained and tested on a very very small corpus... bigger datasets coming soon!_\n",
    "\n",
    "__`part e starts here:`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/root/HW2/smooth-sort-output': No such file or directory\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.15.1.jar] /tmp/streamjob62723252219042272.jar tmpDir=null\n",
      "19/01/17 00:57:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:57:06 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/01/17 00:57:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "19/01/17 00:57:08 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "19/01/17 00:57:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1547262255608_0044\n",
      "19/01/17 00:57:09 INFO impl.YarnClientImpl: Submitted application application_1547262255608_0044\n",
      "19/01/17 00:57:09 INFO mapreduce.Job: The url to track the job: http://docker.w261:8088/proxy/application_1547262255608_0044/\n",
      "19/01/17 00:57:09 INFO mapreduce.Job: Running job: job_1547262255608_0044\n",
      "19/01/17 00:57:19 INFO mapreduce.Job: Job job_1547262255608_0044 running in uber mode : false\n",
      "19/01/17 00:57:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/01/17 00:57:28 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "19/01/17 00:57:30 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/01/17 00:57:40 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/01/17 00:57:40 INFO mapreduce.Job: Job job_1547262255608_0044 completed successfully\n",
      "19/01/17 00:57:40 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=391054\n",
      "\t\tFILE: Number of bytes written=1231327\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=258504\n",
      "\t\tHDFS: Number of bytes written=381936\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14545\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=8124\n",
      "\t\tTotal time spent by all map tasks (ms)=14545\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8124\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14545\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8124\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14894080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=8318976\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4556\n",
      "\t\tMap output records=4556\n",
      "\t\tMap output bytes=381936\n",
      "\t\tMap output materialized bytes=391060\n",
      "\t\tInput split bytes=236\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4556\n",
      "\t\tReduce shuffle bytes=391060\n",
      "\t\tReduce input records=4556\n",
      "\t\tReduce output records=4556\n",
      "\t\tSpilled Records=9112\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tCPU time spent (ms)=5230\n",
      "\t\tPhysical memory (bytes) snapshot=793804800\n",
      "\t\tVirtual memory (bytes) snapshot=4118691840\n",
      "\t\tTotal committed heap usage (bytes)=625999872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=258268\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=381936\n",
      "19/01/17 00:57:40 INFO streaming.StreamJob: Output directory: /user/root/HW2/smooth-sort-output\n"
     ]
    }
   ],
   "source": [
    "# part e - write your Hadoop job here (sort smoothed model on P(word|class))\n",
    "\n",
    "# copying from Local to HDFS\n",
    "!hdfs dfs -mkdir {HDFS_DIR}/Smoothed\n",
    "!hdfs dfs -copyFromLocal NaiveBayes/Smoothed/NBmodel.txt {HDFS_DIR}/Smoothed/NBmodel.txt\n",
    "\n",
    "# clear output directory\n",
    "!hdfs dfs -rm -r {HDFS_DIR}/smooth-sort-output\n",
    "\n",
    "# hadoop job\n",
    "!hadoop jar {JAR_FILE} \\\n",
    "  -D stream.num.map.output.key.fields=6 \\\n",
    "  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "  -D mapreduce.partition.keycomparator.options=\"-k3,3 -k4,4nr\" \\\n",
    "  -files NaiveBayes/model_sort_mapper.py \\\n",
    "  -mapper model_sort_mapper.py \\\n",
    "  -reducer /bin/cat \\\n",
    "  -input {HDFS_DIR}/Smoothed/NBmodel.txt \\\n",
    "  -output {HDFS_DIR}/smooth-sort-output \\\n",
    "  -cmdenv PATH={PATH}\n",
    "\n",
    "# retrieve results locally\n",
    "!hdfs dfs -cat {HDFS_DIR}/smooth-sort-output/part-000* > NaiveBayes/Smoothed/sorted_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Top 10 words in Ham Enron Emails ==========\n",
      "ClassPriors\t0.5875\n",
      "ect\t0.023473306082001735\n",
      "and\t0.01604112473677691\n",
      "hou\t0.0126347082868822\n",
      "in\t0.009971509971509971\n",
      "for\t0.00922829183698749\n",
      "on\t0.007617985878855444\n",
      "enron\t0.007246376811594203\n",
      "i\t0.007060572277963582\n",
      "will\t0.007060572277963582\n",
      "awk: (FILENAME=NaiveBayes/Smoothed/sorted_results.txt FNR=693) fatal: print to \"standard output\" failed (Broken pipe)\n",
      "========== Top 10 words in Spam Enron Emails ==========\n",
      "the\t0.029726581997670677\n",
      "to\t0.023348677278021184\n",
      "a\t0.015251511286118352\n",
      "your\t0.01508513116299706\n",
      "you\t0.014031390383228884\n",
      "of\t0.014031390383228884\n",
      "it\t0.0066552049248516446\n",
      "com\t0.006045144473406911\n",
      "that\t0.005601464145083467\n",
      "or\t0.004935943652598303\n",
      "awk: (FILENAME=NaiveBayes/Smoothed/sorted_results.txt FNR=2524) fatal: print to \"standard output\" failed (Broken pipe)\n"
     ]
    }
   ],
   "source": [
    "# part e - print top words in each class\n",
    "\n",
    "print('='*10,f'Top 10 words in Ham Enron Emails','='*10)\n",
    "!awk '$3 == \"ham\" {print $1 \"\\t\" $4}' NaiveBayes/Smoothed/sorted_results.txt | head -n 10 \n",
    "\n",
    "print('='*10,f'Top 10 words in Spam Enron Emails','='*10)\n",
    "!awk '$3 == \"spam\" {print $1 \"\\t\" $4}' NaiveBayes/Smoothed/sorted_results.txt | head -n 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 ends here, please refer to the `README.md` for submission instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
