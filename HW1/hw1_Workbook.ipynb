{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Intro to the Map Reduce Paradigm  \n",
    "__ `MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Spring 2018`__\n",
    "\n",
    "Welcome to Machine Learning at Scale! This first homework assignment introduces one of the core strategies in distributed processing: divide and conquer. We'll use the simplest of tasks, word counting, to illustrate the difference between a scalable and non-scalable algorithm. You will be working with the text of _Alice in Wonderland_ to put these ideas into practice using Python and Bash scripting. By the end of this week you should be able to:\n",
    "* ... __describe__ the Bias-Variance tradeoff as it applies to Machine Learning.\n",
    "* ... __explain__ why we consider word counting to be an \"Embarrassingly Parallel\" task.\n",
    "* ... __estimate__ the runtime of embarrassingly parallel tasks using \"back of the envelope\" calculations.\n",
    "* ... __implement__ a Map Reduce algorithm using the Command Line.\n",
    "* ... __set-up__ a Docker container and know why we use them for this course.\n",
    "\n",
    "You will also  become familiar (if you aren't already) with `defaultdict`, `re` and `time` in Python, linux piping and sorting, and Jupyter magic commands `%%writefile` and `%%timeit`. __Please refer to the `README` for detailed submission instructions__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Set-Up\n",
    "Before starting your homework run the following cells to confirm your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=6, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm you are running Python 3.6\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a folder for any data you download locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "# NOTE: the contents of this directory will be ignored by git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Introductions\n",
    "\n",
    "`The Caterpillar and Alice looked at each other for some time in silence: at last the Caterpillar took the hookah out of its mouth, and addressed her in a languid, sleepy voice. \"Who are you?\" said the Caterpillar.`   \n",
    "<div style=\"text-align: right\"> -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4 </div>\n",
    "\n",
    "\n",
    "__Tell us about yourself! Briefly describe where you live, how far along you are in MIDS, what other classes you are taking and what you want to get out of w261.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I'm a postdoctoral fellow at the Rockefeller University where I'm focused on identifying a compound that can be developed as a drug to help alleviate the weak immune system. Using a machine learning program such as Relion(R) and in-house GPFS cluster, I became interested in Machine Learning and how to maximize the computation efficiency. This is my 4th semester and I hope to learn machine learning at scale in w261. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Bias - Variance\n",
    "__In 1-2 sentences explain the bias-variance trade off. Describe what it means to \"decompose\" sources of error. How is this used in machine learning?__ Please use mathematical equation(s) to support your explanation. (Use `$` signs to take advantage of $\\LaTeX$ formatting, eg. `$f(x)$` will look like: $f(x)$). Please also cite any sources that informed your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Bias-variance trade off occurs when we develop a predictive model. A simple modelling results in high bias with low variance. A more complex model (eg, polynomial) results in high variance with low bias, due to our model capturing every datapoints in the training set. Compromising one aspect gives rise to an increase in its counterpart, resulting in bias-variance tradeoff situation. In machine learning, bias-variance decomposition is used in reducing error cost and build a better model. The error term can be decomposed as: \n",
    ">  \n",
    "> $Error = Bias^2 + Variance + \\sigma^2$ where $\\sigma^2$ is an inherent error that cannot be reduced.  \n",
    ">  \n",
    "> $E \\big[ (y - \\hat{f}(x))^2 \\big] = E \\big[ \\hat{f}(x) - f(x) \\big]^2 + E \\big[ \\hat{f}(x)^2 - (E[\\hat{f}(x)])^2 \\big] + \\sigma^2$  \n",
    ">  \n",
    "> Therefore,  \n",
    "> Bias = $E \\big[ \\hat{f}(x) - f(x) \\big]^2$  \n",
    "> Variance = $E \\big[ \\hat{f}(x)^2 - (E[\\hat{f}(x)])^2 \\big]$  \n",
    "> where $\\hat{f}(x)$ is our predictive model and $f(x)$ is a true function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Tokenizing\n",
    "A number of our assignments this term will involve extracting information from text. A common preprocessing step when working with raw files is to 'tokenize' (i.e. extract words from) the text. Within the field of Natural Language Processing a lot of thought goes into what specific tokenizing makes most sense for a given task. For example, you might choose to remove punctuation or to consider punctuation symbols  'tokens' in their own right. __In this question you'll use the Python `re` module to create a tokenizer to use when you perform WordCount on the _Alice In Wonderland_ text.__\n",
    "\n",
    "### Q3 Tasks:\n",
    "\n",
    "* __a) short response:__ In the Naive Bayes algorithm (which we'll implement next week), we'll estimate the _likelihood_ of a word by counting the number of times it appears and dividing by the total number of words. In the space provided below give a concrete example of how two different tokenizers could cause us to get two different results on this calculation. [`HINT`: _you should not need to read up on Naive Bayes to answer this question_]  \n",
    "  \n",
    "\n",
    "* __b) short response:__ When tokenizing in this assignment we'll remove punctuation, make everything lowercase, and discard numerical digits. Suppose __`tokenize(x)`__ is a Python function that performs the desired tokenization. What would __`tokenize(\"By-the-bye, what became of Alice's 12 hats?!\")`__ output? Type the answer in the space provided below.\n",
    "\n",
    "\n",
    "* __c) code:__  Fill in the regular expression pattern in the cell labeled `part c` so that the subsequent call to `re.findall(RE_PATTERN, ...)` returns the tokenization described above. [`HINT`: _we've taken care of the lowercase part for you. If regex is new to you, check out the [`re`  documentation](https://docs.python.org/3/library/re.html) or [this PyMOTW tutorial](https://pymotw.com/2/re/)._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Student Answers:\n",
    "> __a)__ Different tokenizer can give different results, depending on the parameters we select. For example, in the following sentence,  \n",
    "> `\"Ken likes to master in machine learning, but he learns that the learning curve is steep.\"`  \n",
    ">  \n",
    "> If we tokenize the sentence using space only, there would be `[\"learning,\", \"learning\"]` (leaving other tokenized words here)  \n",
    "> If we tokenize the sentence using space and comma, there would be only `[\"learning\"]` and we would count `learning` twice and it will change the maximum likelihood in Naive Bayes calculation.  \n",
    "> Even more so, if we tokenize the sentence with additional parameter such as stemming words, we'll get `[\"learn\"]` because all (learning, , learning, learns) will be considered as `learn` and counted 3 times and MLE will change as well.  \n",
    "\n",
    "> __b)__ `[\"by\", \"the\", \"bye\", \"what\", \"became\", \"of\", \"alice\", \"s\", \"hats\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part C - Fill in the regular expression\n",
    "RE_PATTERN = re.compile(\"[a-z]+\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by', 'the', 'bye', 'what', 'became', 'of', 'alice', 's', 'hats']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - DO NOT MODIFY THIS CELL, just run it as is to check your pattern\n",
    "words = re.findall(RE_PATTERN, \"By-the-bye, what became of Alice's 12 hats?!\".lower())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "` \"Please would you tell me\", said Alice, a little timidly, for she was not quite sure whether it was good manners for her to speak first, \"why your cat grins like that?\"`  \n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 4  </div>\n",
    "\n",
    "For the main part of this assignment we'll be working with the free plain text version of _Alice's Adventures in Wonderland_ available from Project Gutenberg. __Use the first two cells below to download this text and preview the first few lines.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  169k  100  169k    0     0   478k      0 --:--:-- --:--:-- --:--:--  480k\n"
     ]
    }
   ],
   "source": [
    "# Download Full text \n",
    "# NOTE: feel free to replace 'curl' with 'wget' or equivalent command of your choice.\n",
    "!curl 'http://www.gutenberg.org/files/11/11-0.txt' -o data/alice.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "���Project Gutenberg���s Alice���s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n"
     ]
    }
   ],
   "source": [
    "# Take a peak at the first few lines\n",
    "!head -n 6 data/alice.txt\n",
    "# NOTE: If you are working in JupyterLab on Docker you may not see the output \n",
    "# below due to an encoding issue... in that case, use a terminal on Docker to \n",
    "# execute this head command and confirm that the file has downloaded properly, \n",
    "# this encoding issue should not affect your work on subsequent HW items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like you to develop a habit of creating small files with simulated data for use in developing, debugging and testing your code. The jupyter magic command `%%writefile` is a convenient way to do this. __Run the following cells to create a test data file for use in our word counting task.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/alice_test.txt\n",
    "This is a small test file. This file is for a test.\n",
    "This small test file has two small lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_test.txt\n"
     ]
    }
   ],
   "source": [
    "# confirm the file was created in the data directory using a grep command:\n",
    "!ls data | grep test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Word Count in Python\n",
    "\n",
    "Over the course of this term you will also become very familiar writing Python programs that read from standard input and using Linux piping commands to run these programs and save their output to file. __In this question you will write a short python script to perform the Word Count task and then run your script on the _Alice in Wonderland_ text__. You can think of this like a 'baseline implementation' that we'll later compare to the parallelized version of the same task.\n",
    "\n",
    "### Q4 Tasks:\n",
    "\n",
    "* __a) short response:__ How is a `defaultdict` different than a regular Python dictionary? Why might a `defaultdict` be particularly convenient in the context of counting word occurrences as you stream over a text file? (`HINT:` _if this Python type is new to you, check out the [documentation](https://docs.python.org/3.3/library/collections.html#defaultdict-objects) and [this brief tutorial](https://www.accelebrate.com/blog/using-defaultdict-python/)_)\n",
    "\n",
    "\n",
    "* __b) code:__ Complete the Python script in the file __`wordCount.py`__. Read the docstrings carefully to be sure you understand the expected behavior of this function. Please do not code outside of the marked location.\n",
    "\n",
    "\n",
    "* __c) testing:__ Run the cell marked `part c` to call your script on the test file we created above. Confirm that your script returns the correct counts for each word by visually comparing the output to the test file. \n",
    "\n",
    "\n",
    "* __d) results:__ When you are confident in your implementation, run the cell marked `part d` to count the number of occurrences of each word in _Alice's Adventures in Wonderland_. In the same cell we'll pipe the output to file. Then use the provided `grep` commands to check your answers.\n",
    "\n",
    "\n",
    "* __e) short response:__ Suppose you decide that you'd really like  a word and its plural (e.g. 'hatter' and 'hatters') to be counted as the same word. After we have run the wordcount would it be more efficient to post-process your output file or discard your output file and start the analysis over with a new tokenizer? Explain your reasoning briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Student Answers:\n",
    "> __a)__ In Python, a conventional dictionary{} requires a new key to be assigned with its value. eg. `myDict['learn'] = 0`  \n",
    "> Using defaultdict{} bypasses this requirement and automatically assigns a new key or even non-existent keys with a default value such as `0`. So we can straight ahead assign the new key such as `myDefaultDict['learn'] += 1` without the `0` assignment in the first place.     \n",
    "\n",
    "> __b-d)__ _Complete the coding portions of this question before answering 'e'._\n",
    "\n",
    "> __e)__ It depends on the granularity of the text document a person wants to keep as intact, eg. keeping multiple steps in-between intact if in case for the future references. If we like both hatter and hatters to be counted as the same word, it would be more efficient to do so on the first fly because it would only take O(1) to process if either hatter of hatters will go to the same bin `hatter`. To post process, it will take another round of O(n) for the entire dictionary. For this small file, it wouldn't make a huge difference. But if the document is large, it would be more efficient to pre-process in the first step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - DO YOUR WORK IN wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this \t 3\n",
      "is \t 2\n",
      "a \t 2\n",
      "small \t 3\n",
      "test \t 3\n",
      "file \t 3\n",
      "for \t 1\n",
      "has \t 1\n",
      "two \t 1\n",
      "lines \t 1\n"
     ]
    }
   ],
   "source": [
    "# part c - DO NOT MODIFY THIS CELL, just run it as is to test your script\n",
    "!python wordCount.py < data/alice_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part d - DO NOT MODIFY THIS CELL, just run it as is to perform the word count.\n",
    "!python wordCount.py < data/alice.txt > data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first 10 words & their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project \t 87\n",
      "gutenberg \t 93\n",
      "s \t 219\n",
      "alice \t 403\n",
      "adventures \t 12\n",
      "in \t 431\n",
      "wonderland \t 8\n",
      "by \t 78\n",
      "lewis \t 4\n",
      "carroll \t 4\n"
     ]
    }
   ],
   "source": [
    "!head data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"alice\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice \t 403\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 403\n",
    "!grep alice data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"hatter\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hatter \t 56\n",
      "hatters \t 1\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 56\n",
    "!grep hatter data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check your results:__ How many times does the word \"queen\" appear in the book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen \t 75\n",
      "queens \t 1\n"
     ]
    }
   ],
   "source": [
    "# EXPECTED OUTPUT: 75\n",
    "!grep queen data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Unix Sorting\n",
    "Another common task in this course's assignments will be to make strategic use of sorting. To illustrate what we mean by 'strategic,' let's return to the (admitedly artificial) scenario posed in Question 4e): _we have a file of word counts but we want to find all pairs of words and plurals (eg. 'queen' and 'queens') to join their counts into a single record_. Suppose we were going to do this by hand. A logical approach would be to start with the first word and read through each subsequent word to check if they are a singular-plural pair, then repeat this process for each word in the file.\n",
    "\n",
    "### Q5 Tasks:\n",
    "* __a) short response + code:__ In the worst case scenario, how many comparisons will we have to make to perform the process described above? Use $\\LaTeX$ formatting to show the calculations that led to your answer. What is the Big O complexity of this \"algorithm\"? [`HINTs:` _To answer the first part of this question you'll want to know how many words are in your_ `alice_counts.txt` _file -- in the space provided below, write a unix command to check. If you need a Big O notation refresher, here's_ a [blog post](https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/), a [cheatsheet](http://bigocheatsheet.com), and a [thorough explanation](http://pages.cs.wisc.edu/~vernon/cs367/notes/3.COMPLEXITY.html).]\n",
    "\n",
    "\n",
    "* __b) short response:__ If the word count file were sorted alphabetically, how would your approach to this task change? How many comparisons would you have to make? What would the Big O complexity of this new strategy be?\n",
    "\n",
    "\n",
    "* __c) short response:__ What is the Big O complexity of the fastest sorting algorithms? When we take the time to sort in to account, does it make sense to alphabetize before combining singular-plural pairs? [`HINT:` _Re: sorting algorithms -- Wikipedia is your friend, as is the cheatsheet from part a._]\n",
    "\n",
    "\n",
    "* __d) very short response:__ What do we mean by 'strategic' when we say 'strategic sorting'?\n",
    "\n",
    "\n",
    "* __e) code:__ Write a unix command to sort your word count file alphabetically. Save (i.e. pipe) the results to `data/alice_counts_A-Z.txt`. [`HINT:` if Unix sort commands are new to you, start with [this biowize blogpost](https://biowize.wordpress.com/2015/03/13/unix-sort-sorting-with-both-numeric-and-non-numeric-keys/) or [this unixschool tutorial](http://www.theunixschool.com/2012/08/linux-sort-command-examples.html)]\n",
    "\n",
    "\n",
    "* __f) code:__ A (slightly) more likely scenario is that we'd want to look at the top 10 most frequent words that appear in the book. Write a unix command to sort your word count file from highest to lowest count. Save (i.e. pipe) your results to `data/alice_counts_sorted.txt`; then run the provided cell to print the top ten words. Compare your output to the expected output we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 Student Answers:\n",
    "> __a)__ Unix word count shows that we have 3009 words in our file. If we were to check the first word with all other words (3008), and check the 2nd word with all other words again (3007) and continue to do so, we will have = n \\* (n - 1)/2 = (3009 * 3008)/2 = 4,525,536 times to check every word in the entire text file. We divide the output by 2 because of the permutation that won't matter to us, eg, comparing between `queen` and `queens` or `queens` and `queen`.  \n",
    "> This gives us the Big O complexity as $O(n^2)$.  \n",
    "\n",
    "> __b)__ If the word count file is already sorted alphabetically, the comparison is significantly reduced to $O(n)$ because each word is only checked against the next word as they are very similar and sit next to each other because of the plural effect 's'. However, in certain situations, we might need to take into account of irregular plural words such as \"goose vs geese\", \"foot vs feet\", \"man vs men\", \"woman vs women\".  \n",
    "\n",
    "> __c)__ The fastest sorting algorithms have complexity of $O(n \\cdot logn)$. If we sort our text file first, it will take $O(n \\cdot logn)$. We then later pair up the nearby words which take $O(n)$. Our sorting and pairing up will therefore take $O(n\\cdot logn)$ and $O(n)$. However since the $O(n)$ will be dwarfed by the bigger O, our total complexity is therefore $O(n \\cdot logn)$. \n",
    "\n",
    "> __d)__ **Strategic** means choosing the best approach to achieve the same result. In this case, our best approach is dictated by Big O complexity and we're trying to achieve as fast as possible.  \n",
    "> $O(n^2)$ = As we want to count both `hatter` and `hatters`, `queen` and `queens` the same word, we could achieve by tokenizing the text file, post-process the tokenized words without sorting.  \n",
    "> $O(n \\cdot logn)$ = pre-sort the tokenized words, pair up the nearby words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3009 data/alice_counts.txt\n"
     ]
    }
   ],
   "source": [
    "# part a - write a unix command to check how many records are in your word count file\n",
    "!wc -l data/alice_counts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part e - unix command to sort your word counts alphabetically \n",
    "!sort -k1,1 data/alice_counts.txt > data/alice_counts_A-Z.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t 690\n",
      "abide \t 2\n",
      "able \t 1\n",
      "about \t 102\n",
      "above \t 3\n",
      "absence \t 1\n",
      "absurd \t 2\n",
      "accept \t 1\n",
      "acceptance \t 1\n",
      "accepted \t 2\n"
     ]
    }
   ],
   "source": [
    "# part e - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_A-Z.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part f - unix command to sort your word counts from highest to lowest count\n",
    "!sort -nr -k2,2 data/alice_counts.txt > data/alice_counts_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the \t 1818\n",
      "and \t 940\n",
      "to \t 809\n",
      "a \t 690\n",
      "of \t 631\n",
      "it \t 610\n",
      "she \t 553\n",
      "i \t 545\n",
      "you \t 481\n",
      "said \t 462\n"
     ]
    }
   ],
   "source": [
    "# part f - DO NOT MODIFY THIS CELL, run it as is to confirm your sort worked\n",
    "!head data/alice_counts_sorted.txt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>expected output for (e):</th>\n",
    "<th>expected output for (f):</th>\n",
    "<tr><td><pre>\n",
    "a          690\n",
    "abide      2\n",
    "able       1\n",
    "about      102\n",
    "above      3\n",
    "absence    1\n",
    "absurd     2\n",
    "accept     1\n",
    "acceptance 1\n",
    "accepted   2\n",
    "</pre></td>\n",
    "<td><pre>\n",
    "the\t 1818\n",
    "and\t 940\n",
    "to\t 809\n",
    "a\t 690\n",
    "of\t 631\n",
    "it\t 610\n",
    "she\t 553\n",
    "i\t 545\n",
    "you\t 481\n",
    "said\t 462\n",
    "</pre></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Parallel Word Count (part 1)\n",
    "What would happen if we tried to run our script on a much larger dataset? For one thing, it would take longer to run. However there is a second concern. The Python object that aggregates our counts (`defaultdict`) exists in memory on the machine running this notebook. If the vocabulary is too large for the memory space available we would crash the notebook. The solution? Divide and Conquer! Instead of running the script on the whole dataset at once, we could split our text up in to smaller 'chunks' and process them independently of each other. __In this question you'll use a bash script to \"parallelize\" your Word Count.__\n",
    "\n",
    "\n",
    "### Q6 Tasks:\n",
    "* __a) read provided code:__ The bash script `pWordCount.sh` takes an input file, splits it into a specified number of 'chunks', and then applies an executable of your choice to each chunk. Read through this code and make sure you understand each line before you proceed. [`HINT:` _For now, ignore the 'student code' section -- you'll use that in part c._]\n",
    "\n",
    "\n",
    "* __b) short response:__ Below we've provided the command to use this script to apply your analysis (`wordCount.py`) to the _Alice_ text in 4 parallel processes. We'll pipe the results into a file called `alice_pCounts.txt.` Run this analysis and compare the count for the word 'alice' to your answer from Question 4. Explain what went wrong and describe what we have to add to `pWordCount.sh` to fix the problem.\n",
    "\n",
    "\n",
    "* __c) code:__ We've provided a python script, `collateCounts.py`, which reads word counts from standard input and combines any duplicates it encounters. Read through this script to be sure you understand how it is written. Then follow the instructions in `pWordCount.sh` to make a one-line modification so that it accepts `collateCount.py` as a 4th argument and uses this script to combine the chunk-ed word counts. Run the cell below to confirm that you now get the correct results for your 'alice' count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 Student Answers:\n",
    "> __b)__ Since the pWordCount.sh script was fed with 4 (m, number of processes), the text file was divided into 4 different chunks and processed accordingly by wordCount.py. To combine the results, we need an additional function which is a reducer (collateCounts.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - make sure your scripts are executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x pWordCount.sh\n",
    "!chmod a+x wordCount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part b - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount.sh 4 'data/alice.txt' 'wordCount.py' > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice \t 111\n",
      "alice \t 127\n",
      "alice \t 122\n",
      "alice \t 43\n"
     ]
    }
   ],
   "source": [
    "# part b - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - make the collate script is executable (RUN THIS CELL AS IS)\n",
    "!chmod a+x collateCounts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - parallel word count on Alice text (RUN THIS CELL AS IS)\n",
    "!./pWordCount.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'collateCounts.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# part c - check alice count (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Parallel Word Count (part 2)\n",
    "Congratulations, you've just implemented a Map-Reduce algorithm! From here on out, we'll refer to the two python scripts you passed to `pWordCount.sh` as '_mapper_' and '_reducer_'. The bash script itself served as our '_framework_' -- it split up the original input, then ___mapped___ our word counting script on to each chunk, then ___aggregated (a.k.a. reduced)___ the resulting files by piping them into our collation script.  Unfortunately, as you may have realized already, there is a major scalability concern with this particular implementation. __In this question you'll fix our implementation of parallel word count so that it will be scalable.__\n",
    "\n",
    "### Q7 Tasks:\n",
    "\n",
    "* __ a) short response:__ What is the potential scalability problem with the provided implementation of `collateCounts.py`? Explain why this supposedly 'parallelized' Word Count wouldn't work on a really large input corpus.  [`HINT:` _see the intro to Q6_]\n",
    "\n",
    "\n",
    "* __ b) code:__ Fortunately, a 'strategic sort' can solve this problem. Read the instructions at the top of `pWordCount.sh` carefully then comment out your solution to `Q6.c` in `pWordCount.sh` and replace it with a line that alphabetically sorts the output from the mappers (`countfiles`) before piping them into the reducer script.\n",
    "\n",
    "\n",
    "* __c) code:__ Rewrite the main part of `collateCounts.py` so that it takes advantage of the sorted input to add duplicate counts without storing the whole vocabulary in memory. Refer to the file docstring for more detailed instructions. Confirm that your implementation works by running it on both the test and true data files.\n",
    "\n",
    "\n",
    "* __ d) short response:__ If you are paying close attention, this rewritten reducer sets us up for a truly scalable solution, but doesn't get us all the way there. In particular, we're still streaming the entire dataset through one reduce script. If the vocabulary is too large to fit on a single computer, we might split the word counts in half after sorting them, then perform the reducing on two separate machines. Explain what could go wrong with this approach. (For now, ignore the question of how we'd sort a dataset that is too large to fit on a single machine and just focus on what might be wrong about the result of this split-in-half reducing).\n",
    "\n",
    "\n",
    "* __ e) short response:__ Can you come up with a different way of splitting up the data that would allow us to perform the reducing on separate machines without needing any postprocessing? This is a theoretical question -- don't worry if you don't know how to implement your idea in a bash script, just describe how you'd want to split the sorted counts into different files to be reduced separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 Student Answers:\n",
    "\n",
    "> __a)__ The collateCounts.py (reducer) also uses defaultdict{}. It will iterate all tokenized words in the text file. If the text file is very large with a bigger corpus of words, even with a parallelized word count approach, it will probably run out of memory for parallel operation. \n",
    "\n",
    "> __b-c)__ _complete the coding portions of this question before answering d and e._\n",
    "\n",
    "> __d)__ Split the word count in half might go wrong if we split the same word into several chunks, the counting will be doubled instead of a true count.  \n",
    "\n",
    "> __e)__ We can split the sorted word count into corresponding number of machines available by alphabetical order. For eg, if there are two machines availabe, we can split 26 alphabets into 26/2 = 13 alphabet on each machine. So machine A will reduce tokenized word from `A - M` and machine B from `N - Z`. If there are 4 machines, 26/4 = 6.5 alphabet each machine. We need to distribute alphabets equally across all 4 machine such as `A - F`, `G - L`, `M - S`, `T - Z`, considering the X,Y,Z words are few, we distribute more than 6 alphabets into the last machine. For me, it will look like we need an additional final step where we combine outputs from all machines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t2\n",
      "file\t3\n",
      "for\t1\n",
      "has\t1\n",
      "is\t2\n",
      "lines\t1\n",
      "small\t3\n",
      "test\t3\n",
      "this\t3\n",
      "two\t1\n"
     ]
    }
   ],
   "source": [
    "# part c - test your code on the test file (RUN THIS CELL AS IS)\n",
    "!./pWordCount.sh 4 'data/alice_test.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'collateCounts.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# part c - run your code on the Alice file (RUN THIS CELL AS IS)\n",
    "!./pWordCount.sh 4 'data/alice.txt' \\\n",
    "                   'wordCount.py' \\\n",
    "                   'collateCounts.py' \\\n",
    "                   > 'data/alice_pCounts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice\t403\n"
     ]
    }
   ],
   "source": [
    "# part c - confirm that your 'alice' count is correct (RUN THIS CELL AS IS)\n",
    "!grep alice data/alice_pCounts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8: Scalability Considerations\n",
    "In your reading for Week 1's live session, [Chapter1, section 2](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf) of _Data Intensive Text Processing with MapReduce_, Lin and Dyer discuss a number of \"Big Ideas\" that underlie large scale processing: __scale \"out,\" not \"up\"; assume failures are common; move processing to the data; process data sequentially and avoid random access; hide system-level details from the application developer; and seamless scalability.__ Part of our work this semester will be to interact with these ideas in a practical way, not just a conceptual one.\n",
    "\n",
    "### Q8 Tasks:\n",
    "* __a) short response:__ What do Lin and Dyer consider the two features of an \"ideal algorithm\" from a scalability perspective?\n",
    "\n",
    "\n",
    "* __b) short response:__ The mapper script below (created on the fly using a little Jupyter magic) will help us illustrate the concept of scalability. Run the provided code which passes this mapper script to our parallel computation 'framework' and runs the 'analysis' on the _Alice_ text file. Note that we've omitted a reducer for simplicity. What do you observe about the time it takes to run this \"algorithm\" when we use 1, 2 and 4 partitions? Does it meet Lin and Dyer's criteria?\n",
    "\n",
    "\n",
    "* __c) short response:__ Let's try something similar with your Word Count analysis. Run the provided code to time your implementation with 1, 2, 4 and 8 partitions. What do you observe about the runtimes? Does this match your expectation? Speculate about why we might be seeing these times. What conclusions should we draw about the scalability of our implementation? [`HINT:` _consider the limitations of both your machine and our implementation... there are some competing forces at work, what are they?_]\n",
    "\n",
    "\n",
    "* __d) BONUS:__ Which components of your Map-Reduce algorithm are affected by a change in the number of partitions? Does increasing the number of partitions increase or decrease the total time spend on each of these portions of the task? What tradeoff does this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 Student Answers:\n",
    "\n",
    "> __a)__ An ideal algorithm should scale accordingly with the amount of data and computational power available. If the amount of data is twice as large, the same algorithm will take twice as long to process. If the computational power is twice as efficient, the same algorithm should not take no more than half as long to process. \n",
    "\n",
    "> __b)__ When the program was run with 1 partition, it took 5.97s, with 2 partitions, it took 3.12s and with 4 partitions, it took 1.95s. As we increase the partitions, the program run time decreases by half which meets Lin and Dyer criteria --> *twice as efficient computation should reduce the runtime by half*. However, if we look carefully, when we partitioned into 4, the runtime was not exactly reduced by half, eg, ~ 6s/4 = 1.5s and the actual runtime was 1.95s. Out of curiosity, I also run with 8 partitions, the run time was 1.31s which theoretically should be = 6s/8 = 0.75s. I suspect Lin and Dyer criteria can be met in the beginning of the partitions, i.e., 1 or 2 steps further down. However as we increase more partitions, the reduction is not as half as we increase the power as twice large. In contrast, the reduction becomes minimal, which to me looks like an exponential decay and not a linear correlation. \n",
    "\n",
    "> __c)__ As we increase partitions using our mapper (wordCount.py), runtime increases instead of decreasing, which doesn't meet Lin and Dyer criteria. Our implmentation did not scale well with the number of partitions we introduced. I suspect this could be due to the underlying infrastructure of the machine and the program written. In our mapper (wordCount.py), each chunk of the data is processed and wait until the next chunk of the data can be processed, reducing the availability of the processors, thereby downtime. From instruction cycle standpoints (Fetch, Decode, Execute), it's better to fetch next instruction while the current instruction is being executed in order to maximize the throughput of the processor. By introducing more partitions, we're giving overheads to our implementation rather than increasing the performance of the algorithm. \n",
    "\n",
    "> __d)__ The 2nd component in the map-reduce algorithm affects the change in the number of partitions, i.e., the reducer component. As we increase the number of partitions, it makes it easier for mapper to handle a smaller chunk of data. However more partitions means more handling in the final reducer component, giving rise to more overheads, which inadvertently increases our runtime instead of decreasing it. As a matter of fact, I did run the word count with our mapper (wordCount.py) without the reducer (collateCounts.py) with 1, 2, 4, 8, 16, 32 partitions. In each iteration, the increase in runtime was minimal, presumably due to the lack of reducer component. But with the reducer, the increase was more pronounced. This can be explained why Hadoop implements shuffling or data transfer even when the mapper is not finished yet completely and data is fed into the reducer on the fly to achieve the maximum efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to create the mapper referenced in `part b`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demo/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "This mapper reads from STDIN and waits 0.001 seconds per line.\n",
    "Its only purpose is to demonstrate one of the scalability ideas.\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "for line in sys.stdin:\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the mapper is executable\n",
    "!chmod a+x demo/mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the next three cells to apply our demo mapper with 1, 2 and 4 partitions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.97 s ± 53.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 1 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 s ± 102 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 2 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95 s ± 71.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 4 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.31 s ± 25.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 8 'data/alice.txt' 'demo/mapper.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the following cells to repeat this process with your word count algorithm.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 ms ± 6.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 1 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 ms ± 12.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 2 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601 ms ± 44.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 4 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 ms ± 40.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 8 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44 s ± 203 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 16 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.08 s ± 247 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "!./pWordCount.sh 32 'data/alice.txt' 'wordCount.py' 'collateCounts.py' > 'data/tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Embarrassingly Parallel\n",
    "`\"If any one of them can explain it,\" said Alice, (she had grown so large in the last few minutes that she wasn’t a bit afraid of interrupting him,) \"I’ll give him sixpence. I don’t believe there’s an atom of meaning in it.\"`\n",
    "<div style=\"text-align: right\">  -- Lewis Carroll, _Alice's Adventures in Wonderland_, Chapter 12  </div>\n",
    "\n",
    "__What do we mean when we call something 'Embarrassingly Parallel'? Does this term describe a 'task'? an 'implementation of a task'? What makes word counting 'embarrassingly parallel'?__ BONUS: define this concept in terms of 'associative' and 'commutative' operations. [`HINT:` _Refer to Week 2 Async, particularly section 2.4 and the assigned_ [reading from EECS](https://patterns.eecs.berkeley.edu/?page_id=37) _about Map-Reduce Patterns_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When an implementation of a task is **embarassingly parallel**, it means we can scale up the entire process by divide and conquer approach. The entire algorithm can be easily parallelized in different machines without ever requiring to communicate in the process, which gives high throughput. Otherwise if the other processing unit needs to wait until it receives its input from the other machine, it will create a downtime and overall efficiency is compromised. This brings us to the concept of **associative** and **commutative** operation.  \n",
    "> \n",
    "> **associative** : (a + b) + c = a + (b + c)  **OR** (a \\*b) \\* c = a \\* (b \\* c)  `[the same order, operation on different set]`  \n",
    "> **commutative** : a + b = b + a **OR** a \\* b = b \\* c  `[order changed]`   \n",
    ">  \n",
    "> As we can see from our above example, our word counting from text meets these two operations: associative and commutative. Depending on how many number of processes we want to partitions, eg, 2 partitions will be `a + b`, 3 partitions will be `a + b + c` in the reducer component, in all of those operations, it does not matter whether `a` and `b` are summed up first or `b` and `c` are summed up first. In the end, they all give the same result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10: Getting Set-up with Docker\n",
    "\n",
    "Starting with next week's assignment we will be using a Docker container to ensure a consistent environment for the class. While it is not _required_ that you complete the assignments using this environment we cannot provide support for individual setups. We have done our best to provide a consistent experience. However there will inevitably be new issues that arise since the Machine Learning content in this course is closely intertwined with rapidly evolving infrastructure and since students come froma a range of backgrounds. We encourage you and your peers to help each other troubleshoot infrastructure challenges using the course slack channel and by submitting formal issues for our attention in the environment repo.\n",
    "\n",
    "### Q10 Tasks:\n",
    "* __a) set up Docker:__ If you haven't already follow the instructions in the the course infrastructure repository: https://github.com/UCB-w261/w261-environment to get set up. Once you're set up, reopen this notebook on the container and run the provided cell to show that you have successfully set it up.\n",
    "\n",
    "\n",
    "* __b) short response:__ At minimum, how much RAM must you allocate to your Docker container to ensure that Hadoop runs properly? [`HINT:` _the answer is in the environment repo's README.md_]\n",
    "\n",
    "\n",
    "* __c) short response:__ What parameter must we add to our Hadoop streaming \n",
    "jobs if we want to use additional python packages inside our MapReduce or MRJob scripts? [`HINT:` _the answer is in the environment repo's README.md... no worries if you don't yet know what a Hadoop MapReduce job is._]\n",
    "\n",
    "\n",
    "* __d) short response:__ What variable allows us to access the Spark Context from within a Jupyter Notebook running on the Docker container? [`HINT:` _the answer is in the environment repo's README.md... no worries if you don't yet know what a Spark Context is._]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 Student Answers:\n",
    "> __b)__ 4GB of RAM with 2CPU\n",
    "\n",
    "> __c)__ `-cmdenv PATH=/opt/anaconda/bin:$PATH`\n",
    "\n",
    "> __d)__ `sc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxr-xr-x   1 root root     0 Jan 12 03:03 .dockerenv\n",
      "-rwxr-xr-x   1 root root     0 Apr  6  2016 .dockerinit\n"
     ]
    }
   ],
   "source": [
    "# part a - show us that you are on Docker! (RUN THIS CELL AS IS)\n",
    "!ls -la / | grep docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you've completed HW1. \n",
    "# Please refer to the README for submission instructions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "297px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "951px",
    "left": "0px",
    "right": "1561px",
    "top": "106px",
    "width": "600px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
